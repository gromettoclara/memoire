
@phdthesis{kristensen_dalto_2022,
	type = {mémoire de master},
	title = {D’{ALTO} à {TEI} : {Modélisation} de transcriptions automatiques pour une pré-éditorialisant des textes},
	shorttitle = {D’{ALTO} à {TEI}},
	url = {https://github.com/kat-kel/TNAH-Memoire},
	language = {fr},
	school = {École Nationale des Chartes},
	author = {Kristensen, Kelly},
	year = {2022},
	keywords = {eda},
}


@misc{rousseau_epicycles_nodate,
	title = {Epicycles de {Ptolémée}},
	url = {https://ressources.univ-lemans.fr/AccesLibre/UM/Pedago/physique/02/divers/ptolemee.html},
	urldate = {2024-03-03},
	author = {Rousseau, Jean-Jacques},
	keywords = {hist astro},
	file = {Epicycles de Ptolémée:/home/clara/Zotero/storage/XNE3FTG7/ptolemee.html:text/html},
}

@misc{raymond_jones_ptolemy_2024,
	title = {Ptolemy},
	url = {https://www.britannica.com/biography/Ptolemy},
	abstract = {Ptolemy was an astronomer, mathematician, and geographer who lived during the 2nd century CE. He is known for his geocentric (Earth-centred) model of the universe.},
	language = {en},
	urldate = {2024-03-03},
	journal = {Encyclopedia Britanica},
	author = {Raymond Jones, Alexander},
	month = feb,
	year = {2024},
	keywords = {hist astro},
	file = {Snapshot:/home/clara/Zotero/storage/K8BFQRA2/George-of-Trebizond.html:text/html},
}

@misc{evans_astronomy_nodate,
	title = {Astronomy - {Ancient} {Greece}, {Stars}, {Planets}},
	url = {https://www.britannica.com/science/astronomy/Ancient-Greece},
	urldate = {2024-03-03},
	journal = {Encyclopedia Britanica},
	author = {Evans, James},
	keywords = {hist astro},
	file = {Astronomy - Ancient Greece, Stars, Planets | Britannica:/home/clara/Zotero/storage/4W7BH99L/Ancient-Greece.html:text/html},
}

@misc{costabel_biographie_nodate,
	title = {Biographie de {Claude} {Ptolémée} (90 env.-env. 168)},
	url = {https://www.universalis.fr/encyclopedie/claude-ptolemee/},
	abstract = {Biographie de CLAUDE PTOLÉMÉE (90 env.-env. 168). Le nom de Ptolémée n'est guère connu aujourd'hui qu'en tant qu'il désigne un système : le système astronomique qui plaçait la Terre immobile au centre du monde et dont la mise en question, de Copernic à Newton, a commandé la révolution...},
	language = {fr-FR},
	urldate = {2024-03-03},
	journal = {Encyclopædia Universalis},
	author = {Costabel, Pierre},
	keywords = {hist astro},
}

@misc{lequeux_systeme_nodate,
	title = {Système géocentrique de {Ptolémée}},
	url = {https://www.universalis.fr/encyclopedie/systeme-geocentrique-de-ptolemee/},
	abstract = {Dans sa Syntaxe mathématique , plus connue sous le titre d' Almageste , et dans laquelle la dernière observation consignée date de 141, Claude Ptolémée ( ii e  siècle) expose l'ensemble des connaissances astronomiques de son époque. Il décrit en particulier le mouvement du Soleil , de la...},
	language = {fr-FR},
	urldate = {2024-03-03},
	journal = {Encyclopædia Universalis},
	author = {Lequeux, James},
	keywords = {hist astro},
	file = {Snapshot:/home/clara/Zotero/storage/8BM7Z426/systeme-geocentrique-de-ptolemee.html:text/html},
}

@book{safran_diagram_2022,
	title = {The {Diagram} as {Paradigm}: {Cross}-{Cultural} {Approaches}},
	isbn = {978-0-88402-486-6},
	shorttitle = {The {Diagram} as {Paradigm}},
	author = {Safran, Linda and Hamburger, Jeffrey and Roxburgh, David},
	month = jul,
	year = {2022},
	keywords = {hist astro},
}

@article{prunet_iiif_2022,
	title = {{IIIF} : découverte et interopérabilité sans frontières des images patrimoniales},
	url = {https://www.culture.gouv.fr/fr/Thematiques/Enseignement-superieur-et-Recherche/La-revue-Culture-et-Recherche/La-recherche-culturelle-a-l-international},
	number = {143},
	urldate = {2024-03-03},
	journal = {Culture et Recherche : La recherche culturelle à l’international},
	author = {Prunet, C. and Bertrand, S. and Chenard, G. and Pillorget, S. and Robineau, Régis},
	year = {2022},
	keywords = {théorie, EDA},
	pages = {111--117},
}

@misc{moiraghi_explorer_2018,
	type = {Billet},
	title = {Explorer des corpus d’images. {L}’{IA} au service du patrimoine},
	url = {https://bnf.hypotheses.org/2809},
	abstract = {Suite aux ateliers « Décrire, transcrire et diffuser un corpus documentaire hétérogène : méthodes, formats, outils » et « Géolocalisation et spatialisation de documents patrimoniaux », une troisième demi-journée d’étude a été organisée dans le cadre du...},
	language = {fr-FR},
	urldate = {2024-03-03},
	journal = {Carnet de la recherche à la Bibliothèque nationale de France},
	author = {Moiraghi, Eleonora},
	month = apr,
	year = {2018},
	doi = {10.58079/m3cp},
	note = {ISSN: 2493-4437},
	keywords = {ia},
	file = {Snapshot:/home/clara/Zotero/storage/NDIQ9J62/2809.html:text/html},
}

@article{klinke_big_2016,
	title = {Big {Image} {Data} within the {Big} {Picture} of {Art} {History}},
	copyright = {Copyright (c) 2016 International Journal for Digital Art History},
	issn = {2363-5401},
	url = {https://journals.ub.uni-heidelberg.de/index.php/dah/article/view/33527},
	doi = {10.11588/dah.2016.2.33527},
	abstract = {Abstrait
					L’usage de l’ordinateur en histoire de l’art modifie l’approche de nos objets de recherche. Aujourd’hui, nous sommes capables de calculer plus d’images qu’un humain ne peut en voir au cours de sa vie. Cela nécessite à son tour une nouvelle définition du rôle du chercheur et des outils utilisés. L'accès à de grandes quantités de données visuelles s'inscrit dans la tradition des méthodes conventionnelles de l'histoire de l'art, mais les augmente également par la quantité. Cet article propose un modèle théorique sur lequel construire une compréhension de la méta-image avec laquelle nous tirons nos conclusions de manière interactive.},
	language = {en},
	number = {2},
	urldate = {2024-03-03},
	journal = {International Journal for Digital Art History},
	author = {Klinke, Harald},
	month = oct,
	year = {2016},
	note = {Number: 2},
	keywords = {ia},
	file = {Full Text PDF:/home/clara/Zotero/storage/9JZE2U9Q/Klinke - 2016 - Big Image Data within the Big Picture of Art Histo.pdf:application/pdf},
}

@article{buttner_cordeep_2022,
	title = {{CorDeep} and the {Sacrobosco} {Dataset}: {Detection} of {Visual} {Elements} in {Historical} {Documents}},
	volume = {8},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2313-433X},
	shorttitle = {{CorDeep} and the {Sacrobosco} {Dataset}},
	url = {https://www.mdpi.com/2313-433X/8/10/285},
	doi = {10.3390/jimaging8100285},
	abstract = {Recent advances in object detection facilitated by deep learning have led to numerous solutions in a myriad of fields ranging from medical diagnosis to autonomous driving. However, historical research is yet to reap the benefits of such advances. This is generally due to the low number of large, coherent, and annotated datasets of historical documents, as well as the overwhelming focus on Optical Character Recognition to support the analysis of historical documents. In this paper, we highlight the importance of visual elements, in particular illustrations in historical documents, and offer a public multi-class historical visual element dataset based on the Sphaera corpus. Additionally, we train an image extraction model based on YOLO architecture and publish it through a publicly available web-service to detect and extract multi-class images from historical documents in an effort to bridge the gap between traditional and computational approaches in historical studies.},
	language = {en},
	number = {10},
	urldate = {2024-03-03},
	journal = {Journal of Imaging},
	author = {Büttner, Jochen and Martinetz, Julius and El-Hajj, Hassan and Valleriani, Matteo},
	month = oct,
	year = {2022},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {eda},
	pages = {285},
	file = {Full Text PDF:/home/clara/Zotero/storage/GN2Y7BUW/Büttner et al. - 2022 - CorDeep and the Sacrobosco Dataset Detection of V.pdf:application/pdf},
}

@article{wevers_visual_2020,
	title = {The visual digital turn: {Using} neural networks to study historical images},
	volume = {35},
	issn = {2055-7671},
	shorttitle = {The visual digital turn},
	url = {https://doi.org/10.1093/llc/fqy085},
	doi = {10.1093/llc/fqy085},
	abstract = {Digital humanities research has focused primarily on the analysis of texts. This emphasis stems from the availability of technology to study digitized text. Optical character recognition allows researchers to use keywords to search and analyze digitized texts. However, archives of digitized sources also contain large numbers of images. This article shows how convolutional neural networks (CNNs) can be used to categorize and analyze digitized historical visual sources. We present three different approaches to using CNNs for gaining a deeper understanding of visual trends in an archive of digitized Dutch newspapers. These include detecting medium-specific features (separating photographs from illustrations), querying images based on abstract visual aspects (clustering visually similar advertisements), and training a neural network based on visual categories developed by domain experts. We argue that CNNs allow researchers to explore the visual side of the digital turn. They allow archivists and researchers to classify and spot trends in large collections of digitized visual sources in radically new ways.},
	number = {1},
	urldate = {2024-03-03},
	journal = {Digital Scholarship in the Humanities},
	author = {Wevers, Melvin and Smits, Thomas},
	month = apr,
	year = {2020},
	keywords = {ia},
	pages = {194--207},
	file = {Full Text PDF:/home/clara/Zotero/storage/8WIV29TF/Wevers et Smits - 2020 - The visual digital turn Using neural networks to .pdf:application/pdf;Snapshot:/home/clara/Zotero/storage/RJ74CABX/5296356.html:text/html},
}

@misc{crawford_excavating_2019,
	title = {Excavating {AI}: {The} {Politics} of {Training} {Sets} for {Machine} {Learning}},
	url = {https://perma.cc/NE8D-P6AW},
	abstract = {An investigation into the politics of training sets, and the fundamental
problems with classifying humans.},
	language = {en-us},
	urldate = {2024-03-03},
	journal = {excavating.ai},
	author = {Crawford, Kate and Paglen, Trevor},
	month = sep,
	year = {2019},
	keywords = {ia},
	file = {Snapshot:/home/clara/Zotero/storage/UYY6XL7Y/NE8D-P6AW.html:text/html},
}

@book{azencott_introduction_2022,
	address = {Malakoff},
	edition = {2e édition},
	series = {Info {Sup}},
	title = {Introduction au machine learning},
	abstract = {Cet ouvrage s’adresse aux étudiantes et étudiants en fin de licence et en master d’informatique ou de maths appliquées, ainsi qu’aux élèves d’école d’ingénieurs. L’apprentissage automatique, ou machine Learning, est une discipline dont les outils puissants permettent aujourd’hui à de nombreux secteurs d’activité de réaliser des progrès spectaculaires grâce à l’exploitation de grands volumes de données. Le but de cet ouvrage est de vous fournir des bases solides sur les concepts et les algorithmes de ce domaine en plein essor. Il vous aidera à identifier les problèmes qui peuvent être résolus par une approche machine learning, à les formaliser, à identifier les algorithmes les mieux adaptés à chaque cas, à les mettre en oeuvre, et enfin à savoir évaluer les résultats obtenus. Les notions de cours sont illustrées et complétées par 85 exercices, tous corrigés},
	language = {fre},
	publisher = {Dunod},
	author = {Azencott, Chloé-Agathe},
	year = {2022},
	keywords = {ia},
}

@book{chollet_apprentissage_2020,
	address = {Saint-Cyr-sur-Loire},
	series = {Les essentiels de l'{IA}},
	title = {L'apprentissage profond avec {Python}},
	abstract = {"L’apprentissage automatique a fait des progrès remarquables au cours des dernières années. Nous sommes passés d’une reconnaissance quasi inutilisable de la parole et des images à une précision quasi humaine, de machines qui ne pouvaient pas battre un joueur de Go un peu expérimenté à la défaite d’un champion du monde. Derrière ces progrès se cache l’apprentissage profond ― une combinaison d’avancées théoriques et pratiques qui permet une multitude d’applications intelligentes jusque-là impossibles à réaliser."},
	language = {fre},
	publisher = {machinelearning.fr},
	author = {Chollet, François and Forien, Jacqueline Isabelle},
	year = {2020},
	keywords = {ia},
}

@misc{carremans_handling_2019,
	title = {Handling overfitting in deep learning models},
	url = {https://towardsdatascience.com/handling-overfitting-in-deep-learning-models-c760ee047c6e},
	abstract = {Overfitting occurs when you achieve a good fit of your model on the training data, while it does not generalize well on new, unseen data…},
	language = {en},
	urldate = {2024-03-05},
	journal = {Medium},
	author = {Carremans, Bert},
	month = jan,
	year = {2019},
	keywords = {ia},
	file = {Snapshot:/home/clara/Zotero/storage/KF6VEMMG/handling-overfitting-in-deep-learning-models-c760ee047c6e.html:text/html},
}

@article{canny_computational_1986,
	title = {A {Computational} {Approach} to {Edge} {Detection}},
	volume = {PAMI-8},
	issn = {1939-3539},
	url = {https://ieeexplore.ieee.org/document/4767851},
	doi = {10.1109/TPAMI.1986.4767851},
	abstract = {This paper describes a computational approach to edge detection. The success of the approach depends on the definition of a comprehensive set of goals for the computation of edge points. These goals must be precise enough to delimit the desired behavior of the detector while making minimal assumptions about the form of the solution. We define detection and localization criteria for a class of edges, and present mathematical forms for these criteria as functionals on the operator impulse response. A third criterion is then added to ensure that the detector has only one response to a single edge. We use the criteria in numerical optimization to derive detectors for several common image features, including step edges. On specializing the analysis to step edges, we find that there is a natural uncertainty principle between detection and localization performance, which are the two main goals. With this principle we derive a single operator shape which is optimal at any scale. The optimal detector has a simple approximate implementation in which edges are marked at maxima in gradient magnitude of a Gaussian-smoothed image. We extend this simple detector using operators of several widths to cope with different signal-to-noise ratios in the image. We present a general method, called feature synthesis, for the fine-to-coarse integration of information from operators at different scales. Finally we show that step edge detector performance improves considerably as the operator point spread function is extended along the edge.},
	number = {6},
	urldate = {2024-03-06},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Canny, John},
	month = nov,
	year = {1986},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {eda},
	pages = {679--698},
	file = {Canny - 1986 - A Computational Approach to Edge Detection.pdf:/home/clara/Zotero/storage/K94L3T5C/Canny - 1986 - A Computational Approach to Edge Detection.pdf:application/pdf;IEEE Xplore Abstract Record:/home/clara/Zotero/storage/4DBGEC49/4767851.html:text/html},
}

@misc{gronne_introduction_2022,
	title = {Introduction to {Embedding}, {Clustering}, and {Similarity}},
	url = {https://towardsdatascience.com/introduction-to-embedding-clustering-and-similarity-11dd80b00061},
	abstract = {Introduction to key elements of ML and Autoencoders: Embedding, Clustering, and Similarity.},
	language = {en},
	urldate = {2024-03-10},
	journal = {Medium},
	author = {Grønne, Mathias},
	month = oct,
	year = {2022},
	keywords = {ia},
	file = {Snapshot:/home/clara/Zotero/storage/7N5H3FC2/introduction-to-embedding-clustering-and-similarity-11dd80b00061.html:text/html},
}

@inproceedings{di_leonardo_visual_2016,
	address = {Kraków},
	title = {Visual {Patterns} {Discovery} in {Large} {Databases} of {Paintings}.},
	url = {https://dh2016.adho.org/abstracts/348},
	urldate = {2024-03-10},
	booktitle = {Digital {Humanities} 2016 : {Conference} {Abstracts}},
	publisher = {Jagiellonian University \& Pedagogical University},
	author = {Di Leonardo, Isabella and Seguin, Benoit and Kaplan, Frédéric},
	year = {2016},
	keywords = {ia},
	pages = {169--172},
	file = {DH 2016 Abstracts:/home/clara/Zotero/storage/CGQU5JL3/348.html:text/html},
}

@misc{farley_multimodal_2024,
	title = {Multimodal embeddings concepts - {Image} {Analysis} 4.0 - {Azure} {AI} services},
	url = {https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/concept-image-retrieval},
	abstract = {Concepts related to image vectorization using the Image Analysis 4.0 API.},
	language = {en-us},
	urldate = {2024-03-10},
	author = {Farley},
	month = feb,
	year = {2024},
	keywords = {ia},
	file = {Snapshot:/home/clara/Zotero/storage/G8PQ6VT6/concept-image-retrieval.html:text/html},
}

@misc{delua_supervised_2021,
	title = {Supervised vs. {Unsupervised} {Learning}: {What}’s the {Difference}?},
	shorttitle = {Supervised vs. {Unsupervised} {Learning}},
	url = {https://www.ibm.com/blog/supervised-vs-unsupervised-learning/www.ibm.com/blog/supervised-vs-unsupervised-learning},
	abstract = {In this article, we’ll explore the basics of two data science approaches: supervised and unsupervised. Find out which approach is right for your situation. The world is getting “smarter” every day, and to keep up with consumer expectations, companies are increasingly using machine learning algorithms to make things easier. You can see them in use […]},
	language = {en-US},
	urldate = {2024-03-11},
	journal = {IBM Blog},
	author = {Delua, Julianna},
	month = mar,
	year = {2021},
	keywords = {ia},
	file = {Snapshot:/home/clara/Zotero/storage/TQBRAZ4Z/supervised-vs-unsupervised-learning.html:text/html},
}

@article{castellano_deep_2022,
	title = {A {Deep} {Learning} {Approach} to {Clustering} {Visual} {Arts}},
	volume = {130},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-022-01664-y},
	doi = {10.1007/s11263-022-01664-y},
	abstract = {Clustering artworks is difficult for several reasons. On the one hand, recognizing meaningful patterns based on domain knowledge and visual perception is extremely hard. On the other hand, applying traditional clustering and feature reduction techniques to the highly dimensional pixel space can be ineffective. To address these issues, in this paper we propose DELIUS: a DEep learning approach to cLustering vIsUal artS. The method uses a pre-trained convolutional network to extract features and then feeds these features into a deep embedded clustering model, where the task of mapping the input data to a latent space is jointly optimized with the task of finding a set of cluster centroids in this latent space. Quantitative and qualitative experimental results show the effectiveness of the proposed method. DELIUS can be useful for several tasks related to art analysis, in particular visual link retrieval and historical knowledge discovery in painting datasets.},
	language = {en},
	number = {11},
	urldate = {2024-03-11},
	journal = {International Journal of Computer Vision},
	author = {Castellano, Giovanna and Vessio, Gennaro},
	month = nov,
	year = {2022},
	keywords = {ia},
	pages = {2590--2605},
	file = {Full Text PDF:/home/clara/Zotero/storage/DIBIS2ZF/Castellano et Vessio - 2022 - A Deep Learning Approach to Clustering Visual Arts.pdf:application/pdf},
}

@article{strien_computer_2022,
	title = {Computer {Vision} for the {Humanities}: {An} {Introduction} to {Deep} {Learning} for {Image} {Classification} ({Part} 2)},
	shorttitle = {Computer {Vision} for the {Humanities}},
	url = {https://programminghistorian.org/en/lessons/computer-vision-deep-learning-pt2},
	language = {en},
	urldate = {2024-03-13},
	journal = {Programming Historian},
	author = {Strien, Daniel van and Beelen, Kaspar and Wevers, Melvin and Smits, Thomas and McDonough, Katherine},
	month = aug,
	year = {2022},
	keywords = {ia},
	file = {Snapshot:/home/clara/Zotero/storage/H5MKHE6D/computer-vision-deep-learning-pt2.html:text/html},
}

@phdthesis{norindr_traitement_2023,
	type = {mémoire de master},
	title = {Le traitement des sources historiques par la vision artificielle : l'exemple des manuscrits d'astronomie de tradition ptoléméenne},
	shorttitle = {Le traitement des sources historiques par la vision artificielle},
	url = {https://dumas.ccsd.cnrs.fr/dumas-04255677},
	abstract = {Ce mémoire a été réalisé dans le cadre du Master Technologies numériques appliquées à l’histoire de l’École nationale des chartes. Il est rédigé suite à un stage de quatre mois à l’Observatoire de Paris au sein du projet EIDA, portant sur les diagrammes astronomiques de tradition ptoléméenne du viiie au xviiie siècle et intégrant des techniques de vision artificielle pour le traitement de ses sources. Ce mémoire expose le développement d’outils pour cette intégration, et la conception d’une chaîne de traitement accompagnée de normes et de méthodes pour l’application d’algorithmes de vision dans le cadre d’un projet de recherche en humanités.},
	language = {fr},
	urldate = {2024-02-02},
	school = {École Nationale des Chartes},
	author = {Norindr, Jade},
	year = {2023},
	keywords = {ia},
	file = {Norindr - Le traitement des sources historiques par la visio.pdf:/home/clara/Zotero/storage/W93FQUDU/Norindr - Le traitement des sources historiques par la visio.pdf:application/pdf},
}

@phdthesis{albouy_mediation_2019,
	school = {École Nationale des Chartes},
	type = {mémoire de master},
	title = {Médiation des données de la recherche : Élaboration d’une plateforme en ligne pour une base de tables astronomiques anciennes},
	shorttitle = {Médiation des données de la recherche},
	language = {fr},
	author = {Albouy, Ségolène},
	year = {2019},
	keywords = {eda},
	file = {Albouy - 2019 - Médiation des données de la recherche  Élaboratio.pdf:/home/clara/Zotero/storage/NU9YAZKB/Albouy - 2019 - Médiation des données de la recherche  Élaboratio.pdf:application/pdf},
}

@misc{monnier_unsupervised_2021,
	title = {Unsupervised {Layered} {Image} {Decomposition} into {Object} {Prototypes}},
	url = {http://arxiv.org/abs/2104.14575},
	doi = {10.48550/arXiv.2104.14575},
	abstract = {We present an unsupervised learning framework for decomposing images into layers of automatically discovered object models. Contrary to recent approaches that model image layers with autoencoder networks, we represent them as explicit transformations of a small set of prototypical images. Our model has three main components: (i) a set of object prototypes in the form of learnable images with a transparency channel, which we refer to as sprites; (ii) differentiable parametric functions predicting occlusions and transformation parameters necessary to instantiate the sprites in a given image; (iii) a layered image formation model with occlusion for compositing these instances into complete images including background. By jointly learning the sprites and occlusion/transformation predictors to reconstruct images, our approach not only yields accurate layered image decompositions, but also identifies object categories and instance parameters. We first validate our approach by providing results on par with the state of the art on standard multi-object synthetic benchmarks (Tetrominoes, Multi-dSprites, CLEVR6). We then demonstrate the applicability of our model to real images in tasks that include clustering (SVHN, GTSRB), cosegmentation (Weizmann Horse) and object discovery from unfiltered social network images. To the best of our knowledge, our approach is the first layered image decomposition algorithm that learns an explicit and shared concept of object type, and is robust enough to be applied to real images.},
	urldate = {2024-04-05},
	publisher = {arXiv},
	author = {Monnier, Tom and Vincent, Elliot and Ponce, Jean and Aubry, Mathieu},
	month = aug,
	year = {2021},
	note = {arXiv:2104.14575 [cs]},
	keywords = {eda},
	annote = {Comment: Accepted at ICCV 2021. Project webpage: https://imagine.enpc.fr/{\textasciitilde}monniert/DTI-Sprites},
	file = {arXiv Fulltext PDF:/home/clara/Zotero/storage/9UJJCIHU/Monnier et al. - 2021 - Unsupervised Layered Image Decomposition into Obje.pdf:application/pdf;arXiv.org Snapshot:/home/clara/Zotero/storage/PXV9F3PM/2104.html:text/html},
}

@book{mercier_studies_2004,
	title = {Studies on the tranmission of medieval mathematical astronomy},
	url = {https://data.bnf.fr/temp-work/10f87cf325908d90792f9f269c04ee41/},
	abstract = {Toutes les informations de la Bibliothèque Nationale de France sur : Studies on the tranmission of medieval mathematical astronomy - Raymond Mercier},
	language = {fr},
	urldate = {2024-04-06},
	author = {Mercier, Raymond},
	year = {2004},
	keywords = {hist astro},
	file = {Snapshot:/home/clara/Zotero/storage/5VKLP2G3/10f87cf325908d90792f9f269c04ee41.html:text/html},
}

@article{de_young_editing_2014,
	title = {Editing a collection of diagrames ascribed  to al-Ḥajjāj: an initial case study},
	volume = {15},
	journal = {SCIAMVS},
	author = {De Young, Greg},
	year = {2014},
	keywords = {edit},
	pages = {171--238},
	file = {De Young - 2014 - Editing a collection of diagrames ascribed  to al-.pdf:/home/clara/Zotero/storage/SRHJ6W78/De Young - 2014 - Editing a collection of diagrames ascribed  to al-.pdf:application/pdf},
}

@article{raynaud_building_2014,
	title = {Building the stemma codicum from geometric diagrams: {A} treatise on optics by {Ibn} al-{Haytham} as a test case},
	volume = {68},
	issn = {0003-9519},
	shorttitle = {Building the stemma codicum from geometric diagrams},
	url = {https://www.jstor.org/stable/24569630},
	abstract = {In view of the progress made in recent decades in the fields of stemmatology and the analysis of geometric diagrams, the present article explores the possibility of establishing the stemma codicum of a handwritten tradition from geometric diagrams alone. This exploratory method is tested on Ibn al-Haytham's Epistle on the Shape of the Eclipse, because this work has not yet been issued in a critical edition. Separate stemmata were constructed on the basis of the diagrams and the text, and a comparison showed no major differences. The greater reliability of a stemma codicum constructed on the basis of the diagrams rather than the text of a mathematical work is discussed, and preliminary conclusions are drawn.},
	number = {2},
	urldate = {2024-04-07},
	journal = {Archive for History of Exact Sciences},
	author = {Raynaud, Dominique},
	year = {2014},
	note = {Publisher: Springer},
	keywords = {edit},
	pages = {207--239},
	file = {JSTOR Full Text PDF:/home/clara/Zotero/storage/RYZAMMQT/Raynaud - 2014 - Building the stemma codicum from geometric diagram.pdf:application/pdf},
}

@article{jardine_critical_2010,
	title = {Critical {Editing} of {Early}-{Modern} {Astronomical} {Diagrams}},
	volume = {41},
	issn = {0021-8286},
	url = {https://doi.org/10.1177/002182861004100307},
	doi = {10.1177/002182861004100307},
	language = {en},
	number = {3},
	urldate = {2024-04-07},
	journal = {Journal for the History of Astronomy},
	author = {Jardine, Boris and Jardine, Nicholas},
	month = aug,
	year = {2010},
	note = {Publisher: SAGE Publications Ltd},
	keywords = {edit},
	pages = {393--414},
	file = {Jardine et Jardine - 2010 - Critical Editing of Early-Modern Astronomical Diag.pdf:/home/clara/Zotero/storage/AFJYMEMI/Jardine et Jardine - 2010 - Critical Editing of Early-Modern Astronomical Diag.pdf:application/pdf},
}

@misc{noauthor_intuitive_2016,
	title = {An {Intuitive} {Explanation} of {Convolutional} {Neural} {Networks}},
	url = {https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/},
	abstract = {What are Convolutional Neural Networks and why are they important? Convolutional Neural Networks (ConvNets or CNNs) are a category of Neural Networks that have proven very effective in areas such a…},
	language = {en},
	urldate = {2024-04-11},
	journal = {the data science blog},
	month = aug,
	year = {2016},
	keywords = {ia},
	file = {Snapshot:/home/clara/Zotero/storage/DDX6TXUU/intuitive-explanation-convnets.html:text/html},
}

@misc{noauthor_quick_2016,
	title = {A {Quick} {Introduction} to {Neural} {Networks}},
	url = {https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/},
	abstract = {An Artificial Neural Network (ANN) is a computational model that is inspired by the way biological neural networks in the human brain process information. Artificial Neural Networks have generated …},
	language = {en},
	urldate = {2024-04-15},
	journal = {the data science blog},
	month = aug,
	year = {2016},
	keywords = {ia},
	file = {Snapshot:/home/clara/Zotero/storage/LCNMCZAZ/quick-intro-neural-networks.html:text/html},
}

@misc{zhang_dino_2022,
	title = {{DINO}: {DETR} with {Improved} {DeNoising} {Anchor} {Boxes} for {End}-to-{End} {Object} {Detection}},
	shorttitle = {{DINO}},
	url = {http://arxiv.org/abs/2203.03605},
	doi = {10.48550/arXiv.2203.03605},
	abstract = {We present DINO ({\textbackslash}textbf\{D\}ETR with {\textbackslash}textbf\{I\}mproved de{\textbackslash}textbf\{N\}oising anch{\textbackslash}textbf\{O\}r boxes), a state-of-the-art end-to-end object detector. \% in this paper. DINO improves over previous DETR-like models in performance and efficiency by using a contrastive way for denoising training, a mixed query selection method for anchor initialization, and a look forward twice scheme for box prediction. DINO achieves \$49.4\$AP in \$12\$ epochs and \$51.3\$AP in \$24\$ epochs on COCO with a ResNet-50 backbone and multi-scale features, yielding a significant improvement of \${\textbackslash}textbf\{+6.0\}\${\textbackslash}textbf\{AP\} and \${\textbackslash}textbf\{+2.7\}\${\textbackslash}textbf\{AP\}, respectively, compared to DN-DETR, the previous best DETR-like model. DINO scales well in both model size and data size. Without bells and whistles, after pre-training on the Objects365 dataset with a SwinL backbone, DINO obtains the best results on both COCO {\textbackslash}texttt\{val2017\} (\${\textbackslash}textbf\{63.2\}\${\textbackslash}textbf\{AP\}) and {\textbackslash}texttt\{test-dev\} ({\textbackslash}textbf\{\${\textbackslash}textbf\{63.3\}\$AP\}). Compared to other models on the leaderboard, DINO significantly reduces its model size and pre-training data size while achieving better results. Our code will be available at {\textbackslash}url\{https://github.com/IDEACVR/DINO\}.},
	urldate = {2024-04-20},
	publisher = {arXiv},
	author = {Zhang, Hao and Li, Feng and Liu, Shilong and Zhang, Lei and Su, Hang and Zhu, Jun and Ni, Lionel M. and Shum, Heung-Yeung},
	month = jul,
	year = {2022},
	note = {arXiv:2203.03605 [cs]},
	keywords = {eda},
	file = {arXiv Fulltext PDF:/home/clara/Zotero/storage/YE95GWIY/Zhang et al. - 2022 - DINO DETR with Improved DeNoising Anchor Boxes fo.pdf:application/pdf;arXiv.org Snapshot:/home/clara/Zotero/storage/7K3SANNX/2203.html:text/html},
}

@book{sinatra_pratiques_2014,
	address = {Montréal},
	series = {Parcours numérique},
	title = {Pratiques de l’édition numérique},
	copyright = {https://www.openedition.org/12554},
	isbn = {978-2-8218-5072-9},
	url = {https://books.openedition.org/pum/317},
	language = {fr},
	urldate = {2024-04-21},
	publisher = {Presses de l’Université de Montréal},
	author = {Sinatra, Michaël E. and Vitali-Rosati, Marcello},
	year = {2014},
	doi = {10.4000/books.pum.317},
	note = {Code: Pratiques de l’édition numérique},
	keywords = {edit},
	file = {Texte intégral:/home/clara/Zotero/storage/QKE8SZYF/Sinatra et Vitali-Rosati - 2014 - Chapitre 3. Histoire des humanités numériques.pdf:application/pdf},
}

@book{alessi_editions_2023,
	address = {Montréal},
	series = {Parcours numériques},
	title = {Éditions critiques numériques: entre tradition et changement de paradigme},
	isbn = {978-2-7606-4762-6},
	shorttitle = {{EDITIONS} {CRITIQUES} {NUMERIQUES}},
	url = {http://parcoursnumeriques-pum.ca/12-editionscritiques/},
	language = {fr},
	publisher = {Presses de l'Université de Montréal},
	editor = {Alessi, Robert and Vitali-Rosati, Marcello},
	year = {2023},
	keywords = {edit},
	file = {Alessi et Vitali-Rosati - 2023 - Éditions critiques numériques entre tradition et .pdf:/home/clara/Zotero/storage/SCIQY6Y9/Alessi et Vitali-Rosati - 2023 - Éditions critiques numériques entre tradition et .pdf:application/pdf},
}

@book{epron_ledition_2018,
	address = {Paris},
	series = {Repères},
	title = {L'édition à l'ère numérique},
	isbn = {978-2-7071-9935-5},
	url = {https://papyrus.bib.umontreal.ca/xmlui/handle/1866/20642},
	abstract = {Le numérique est en train de remodeler l’ensemble du processus de production du savoir, de validation des contenus et de diffusion des connaissances. En cause : l’émergence de nouveaux outils et de nouvelles pratiques d’écriture et de lecture, mais aussi un changement plus global que l’on pourrait qualifier de culturel.
Les éditeurs ont posé en termes tantôt apocalyptiques tantôt technophiles un grand nombre de questions, notamment sur l’avenir du livre, les modes d’accès à la connaissance, la légitimation des contenus en ligne et les droits d’auteur. Cet ouvrage propose un état des lieux de l’impact effectif des mutations technologiques sur l’édition, à partir de trois fonctions principales des instances éditoriales : la production des contenus, leur circulation et leur légitimation.
Cet ouvrage combine une approche académique de compréhension des modèles, une observation empirique des pratiques et usages et une analyse des logiques stratégiques déployées dans ce secteur.},
	language = {French},
	publisher = {La Découverte},
	author = {Epron, Benoît and Vitali-Rosati, Marcello},
	year = {2018},
	keywords = {edit},
}

@book{charniak_introduction_2021,
	title = {Introduction au {Deep} {Learning}},
	isbn = {978-2-10-082578-3},
	abstract = {Le deep learning est une forme avancée et plus complexe du machine learning qui fait appel à des réseaux neuronaux à plusieurs couches.Ce manuel d’apprentissage synthétique, avec cours et exercices, s'appuie sur des exemples d’écriture de programmes d’intelligence artificielle dans des domaines comme la vision par ordinateur, la compréhension des langages naturels ou l’apprentissage par renforcement.Cet apprentissage du deep learning se fait en écrivant des programmes avec TensorFlow, framework open source de machine learning.L’auteur est un chercheur en IA de longue date et un enseignant. Dans ce livre il incite ses étudiants et ses lecteurs à appliquer sa méthode qui est "d' apprendre en programmant ».Chaque chapitre propose un projet, des exercices et des lectures complémentaires.Ce cours d’initiation comporte une quarantaine d’exercices dont la moitié sont corrigés.},
	language = {fr},
	publisher = {Dunod},
	author = {Charniak, Eugene},
	month = jan,
	year = {2021},
	keywords = {ia},
}

@misc{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2024-05-05},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2010.11929 [cs]},
	keywords = {eda},
	annote = {Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision\_transformer. ICLR camera-ready version with 2 small modifications: 1) Added a discussion of CLS vs GAP classifier in the appendix, 2) Fixed an error in exaFLOPs computation in Figure 5 and Table 6 (relative performance of models is basically not affected)},
	file = {arXiv Fulltext PDF:/home/clara/Zotero/storage/GEYCJPDL/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf;arXiv.org Snapshot:/home/clara/Zotero/storage/2H6CXS5F/2010.html:text/html},
}

@misc{noauthor_gallicorpora_nodate,
	title = {Gallicorpora},
	url = {https://github.com/Gallicorpora},
	abstract = {Gallicorpora has 13 repositories available. Follow their code on GitHub.},
	language = {en},
	urldate = {2024-05-08},
	journal = {GitHub},
	keywords = {eda},
	file = {Snapshot:/home/clara/Zotero/storage/U4SFAIHW/Gallicorpora.html:text/html},
}

@inproceedings{pinche_between_2022,
	address = {Newcastle, United Kingdom},
	title = {Between automatic and manual encoding},
	url = {https://hal.science/hal-03780302},
	doi = {10.5281/zenodo.7092214},
	abstract = {Cultural heritage institutions today aim to digitise their collections of prints and
manuscripts (Bermès 2020) and are generating more and more digital images (Gray
2009). To enrich these images, many institutions work with standardised formats such as
IIIF, preserving as much of the source’s information as possible. To take full advantage of
textual documents, an image alone is not enough. Thanks to automatic text recognition
technology, it is now possible to extract images’ content on a large scale. The TEI seems
to provide the perfect format to capture both an image’s formal and textual data (Janès
et al. 2021). However, this poses a problem. To ensure compatibility with a range of
use cases, TEI XML files must guarantee IIIF or RDF exports and therefore must be
based on strict data structures that can be automated. But a rigid structure contradicts
the basic principles of philology, which require maximum flexibility to cope with various
situations. The solution proposed by the Gallic(orpor)a project1 attempted to deal with such a
contradiction, focusing on French historical documents produced between the 15th and
the 18th c. It aims to enrich the digital facsimiles distributed by the French National
Library (BnF).},
	urldate = {2024-05-08},
	booktitle = {{TEI} 2022 conference : {Text} as data},
	author = {Pinche, Ariane and Christensen, Kelly and Gabay, Simon},
	month = sep,
	year = {2022},
	keywords = {eda},
	file = {HAL PDF Full Text:/home/clara/Zotero/storage/56VQGVWS/Pinche et al. - 2022 - Between automatic and manual encoding.pdf:application/pdf},
}

@misc{gabay_gallicorpor_2022,
	title = {Gallic(orpor)a : {Processing} {Gallica}'s historical sources},
	shorttitle = {Gallic(orpor)a},
	url = {https://hal.science/hal-03819326},
	abstract = {The apparition of digital libraries have brought together unprecedented amounts of data and provided human sciences the means with which to integrate a new scientific paradigm. Such a transition implies overcoming a technological hurdle, particularly for historical documents which pose a significant number of challenges : manuscript documentation, difficult-to-read hands, variation-rich varieties of language. . . The Gallic(orpor)a project tries to overcome these obstacles.},
	urldate = {2024-05-08},
	author = {Gabay, Simon and Pinche, Ariane and Christensen, Kelly},
	month = sep,
	year = {2022},
	note = {Publisher: Université de Genève
Published: UNIGE Data Science Day},
	keywords = {eda},
	annote = {Poster},
	file = {HAL PDF Full Text:/home/clara/Zotero/storage/HGSLEJU3/Gabay et al. - 2022 - Gallic(orpor)a  Processing Gallica's historical s.pdf:application/pdf},
}

@article{bermes_patrimoine_2020,
	title = {Le patrimoine numérique national à l’heure de l’intelligence artificielle},
	volume = {1},
	url = {https://roia.centre-mersenne.org/item/ROIA_2020__1_1_89_0},
	doi = {10.5802/roia.5},
	abstract = {In a context of increasing volumes of data and reduced processing times, the National Library of France is facing several challenges and developments. In order to collect, preserve, describe and enable the study of massive and heterogeneous data sets, the Library uses not only methods of information sciences but also techniques developed in the field of computer science, especially in artificial intelligence. This growing need to convene complementary skills, combined with the research opportunities opened by these digital collections, has led the Library to create a space for supporting digital humanities.},
	language = {fr},
	number = {1},
	urldate = {2024-05-11},
	journal = {Revue Ouverte d'Intelligence Artificielle},
	author = {Bermès, Emmanuelle and Moiraghi, Eleonora},
	month = jul,
	year = {2020},
	keywords = {ia},
	pages = {89--109},
	file = {Bermès et Moiraghi - 2020 - Le patrimoine numérique national à l’heure de l’in.pdf:/home/clara/Zotero/storage/IQG9T8ZL/Bermès et Moiraghi - 2020 - Le patrimoine numérique national à l’heure de l’in.pdf:application/pdf},
}

@inproceedings{christensen_gallicorpor_2022,
	address = {Paris, France},
	title = {Gallic(orpor)a: {Traitement} des sources textuelles en diachronie longue de {Gallica}},
	shorttitle = {Gallic(orpor)a},
	url = {https://hal.science/hal-03716534},
	urldate = {2024-05-11},
	booktitle = {{DataLab} de la {BnF}},
	author = {Christensen, Kelly and Pinche, Ariane and Gabay, Simon},
	month = jun,
	year = {2022},
	keywords = {eda},
	annote = {https://bnf.hypotheses.org/13405},
	file = {HAL PDF Full Text:/home/clara/Zotero/storage/ENQV6CTD/Christensen et al. - 2022 - Gallic(orpor)a Traitement des sources textuelles .pdf:application/pdf},
}

@misc{noauthor_enhancing_nodate,
	title = {Enhancing {Heritage} {Image} {Databases}},
	url = {https://anr.fr/Project-ANR-17-CE23-0008},
	abstract = {In recent years, computer vision has made several breakthroughs by using very large databases to train deep Convolutional Neural Networks (CNNs). In parallel, a lot of efforts have been invested to digitalize heritage artifacts, such as museum collections or archive images, that are now publicly accessible. CNNs have been successfully tested on these heritage data, but mainly for standard classification tasks, inside a closed database. On the contrary, this project aims at using of the most recent advances in computer vision, and in particular in deep learning, to develop innovative applications that are made possible by the availability of large databases. In particular, we will target tasks such as invariant pattern discovery in large image databases and 3D reconstruction from historical depictions.{\textless}br /{\textgreater}{\textless}br /{\textgreater}Breakthrough progress on these problems would have profound implications both in human sciences such as art history and archaeology, and in their understanding by the public.},
	language = {en},
	urldate = {2024-05-11},
	journal = {Agence nationale de la recherche},
	keywords = {eda},
	file = {Snapshot:/home/clara/Zotero/storage/PRSRR5EH/Project-ANR-17-CE23-0008.html:text/html},
}

@misc{noauthor_vision_nodate,
	title = {Vision artificielle et analyse {Historique} de la circulation de l'illustration {Scientifique}},
	url = {https://anr.fr/Projet-ANR-21-CE38-0008},
	abstract = {VHS réunit des chercheurs en histoire des sciences et en vision artificielle pour concevoir un instrument d’étude historique inédit de la circulation des savoirs scientifiques. L’objectif est de développer des méthodes d’apprentissage non ou faiblement supervisées permettant d’étudier les modalités d’évolution et de transformation des images dans des corpus scientifiques d’envergure du Moyen-Âge et de la période moderne.},
	language = {fr},
	urldate = {2024-05-12},
	journal = {Agence nationale de la recherche},
	keywords = {eda},
	file = {Snapshot:/home/clara/Zotero/storage/YP5RPAW5/Projet-ANR-21-CE38-0008.html:text/html},
}

@misc{noauthor_vhs_nodate,
	title = {{VHS} {Project} : {Presentation}},
	url = {https://vhs.hypotheses.org/presentation},
	abstract = {The VHS project proposes a new approach to the historical study of the circulation of scientific knowledge based on new methods of illustration analysis. Thanks to the recent developments in AI, and Computer Vision,...},
	language = {en-US},
	urldate = {2024-05-12},
	journal = {VHS project},
	keywords = {eda},
}

@article{fouad_computer_2023,
	title = {Computer {Vision} and {Historical} {Scientific} {Illustrations}},
	abstract = {The VHS project (computer Vision and Historical analysis of Scientific illustration circulation) proposes a new approach to the historical study of the circulation of scientific knowledge based on new methods of illustration analysis. Our contributions in this paper are twofold. First, we present a semi-automatic interactive pipeline for scientific illustration extraction that allows and incorporates expert feedback from historians. Second, we introduce a new dataset of scientific illustrations from the Middle Ages to the modern era consisting of 8k illustrations validated by historians and a total number of 235k illustrations obtained from 405k corpora pages. We further discuss our current research for identifying a series of related illustrations from this data.},
	language = {en},
	author = {Fouad, AOUINTI and Sonat, BALTACI Zeynep and Mathieu, AUBRY and Alexandre, GUILBAUD and Stavros, LAZARIS},
	year = {2023},
	keywords = {eda},
	file = {Fouad et al. - 2023 - Computer Vision and Historical Scientific Illustra.pdf:/home/clara/Zotero/storage/V22CEG4A/Fouad et al. - 2023 - Computer Vision and Historical Scientific Illustra.pdf:application/pdf;Fouad et al. - 2023 - Computer Vision and Historical Scientific Illustra.pdf:/home/clara/Zotero/storage/YMEJXSFY/Fouad et al. - 2023 - Computer Vision and Historical Scientific Illustra.pdf:application/pdf},
}

@misc{kaoua_image_2021,
	title = {Image {Collation}: {Matching} illustrations in manuscripts},
	shorttitle = {Image {Collation}},
	url = {http://arxiv.org/abs/2108.08109},
	doi = {10.48550/arXiv.2108.08109},
	abstract = {Illustrations are an essential transmission instrument. For an historian, the first step in studying their evolution in a corpus of similar manuscripts is to identify which ones correspond to each other. This image collation task is daunting for manuscripts separated by many lost copies, spreading over centuries, which might have been completely re-organized and greatly modified to adapt to novel knowledge or belief and include hundreds of illustrations. Our contributions in this paper are threefold. First, we introduce the task of illustration collation and a large annotated public dataset to evaluate solutions, including 6 manuscripts of 2 different texts with more than 2 000 illustrations and 1 200 annotated correspondences. Second, we analyze state of the art similarity measures for this task and show that they succeed in simple cases but struggle for large manuscripts when the illustrations have undergone very significant changes and are discriminated only by fine details. Finally, we show clear evidence that significant performance boosts can be expected by exploiting cycle-consistent correspondences. Our code and data are available on http://imagine.enpc.fr/{\textasciitilde}shenx/ImageCollation.},
	urldate = {2024-05-12},
	publisher = {arXiv},
	author = {Kaoua, Ryad and Shen, Xi and Durr, Alexandra and Lazaris, Stavros and Picard, David and Aubry, Mathieu},
	month = aug,
	year = {2021},
	note = {arXiv:2108.08109 [cs]},
	keywords = {eda},
	annote = {Comment: accepted to ICDAR 2021},
	file = {arXiv Fulltext PDF:/home/clara/Zotero/storage/ULFFCTZU/Kaoua et al. - 2021 - Image Collation Matching illustrations in manuscr.pdf:application/pdf;arXiv.org Snapshot:/home/clara/Zotero/storage/I3SZN62R/2108.html:text/html},
}

@inproceedings{jeanrenaud_affiche_2023,
	address = {Genève, Switzerland},
	title = {L'affiche de film à l'épreuve de la vision par ordinateur},
	url = {https://hal.science/hal-04133342},
	abstract = {Dans l'historiographie des affiches de cinéma, on constate une divergence des chercheurs quant au moment où les compositions exclusivement photographiques deviennent hégémoniques dans la production des affiches. Les études qui jalonnent cette historiographie se basent sur des cas d'étude ou une intuition. Or, mondialement, le passage du dessin à la photographie dans les compositions des affiches de film n'a jamais été étudié à large échelle. Ainsi, cette étude entend, à partir d'un abondant corpus et d'outils numériques, apporter des réponses à cette querelle de spécialistes : à partir de quand la photo inonde-t-elle les affiches de film ?},
	urldate = {2024-05-15},
	booktitle = {Humanistica 2023},
	publisher = {Association francophone des humanités numériques},
	author = {Jeanrenaud, Adrien},
	month = jun,
	year = {2023},
	keywords = {eda},
	file = {HAL PDF Full Text:/home/clara/Zotero/storage/8ZK9G32V/Jeanrenaud - 2023 - L'affiche de film à l'épreuve de la vision par ord.pdf:application/pdf},
}

@inproceedings{champenois_visual_2023,
	address = {Genève, Switzerland},
	series = {Circulations},
	title = {Visual {Contagions}: extraire et tracer la circulation d'images dans des imprimés illustrés},
	shorttitle = {Visual {Contagions}},
	url = {https://hal.science/hal-04108205},
	abstract = {Le projet Visual Contagions vise à pister la circulation internationale des images, à partir notamment d'un corpus mondial d'imprimés illustrés numérisés. Cet article présente la chaîne de pré-traitement des sources du projet, par laquelle sont récupérés des lots d'images proches visuellement-reproductions exactes, images similaires. C'est à partir des résultats de cette chaîne, croisés avec des métadonnées de dates et lieux de publication, qu'une étude sur le temps long et d'échelle mondiale devient possible. La première partie de l'article détaille quels choix algorithmiques ont été faits pour regrouper des illustrations par similarité ; la seconde partie décrit les outils mis en place pour récupérer des données (au format IIIF), extraire les images et les traiter automatiquement, et enfin permettre un post-traitement humain des résultats du classement algorithmique.},
	urldate = {2024-05-15},
	booktitle = {Humanistica 2023},
	publisher = {Association francophone des humanités numériques},
	author = {Champenois, Robin and Joyeux-Prunel, Béatrice},
	month = jun,
	year = {2023},
	keywords = {eda},
	file = {HAL PDF Full Text:/home/clara/Zotero/storage/LJZ5SVP3/Champenois et Joyeux-Prunel - 2023 - Visual Contagions extraire et tracer la circulati.pdf:application/pdf},
}

@inproceedings{carboni_pister_2023,
	address = {Genève, Switzerland},
	series = {Images},
	title = {Pister des circulations visuelles à l'échelle mondiale},
	url = {https://hal.science/hal-04094170},
	abstract = {De la fin du XIXe s. à la généralisation des images sur internet, la presse illustrée a été une force motrice déterminante pour la circulation des images. Elle a touché des publics élargis, diffusant par l’image des idées, des pratiques, des représentations. Mais s’il semble aisé d’étudier la rencontre et le métissage de quelques images, comment comprendre les circulations visuelles à grande échelle? Comment étudier la mondialisation par l’image sans se contenter d’études de cas ? Tel est le défi du projet Visual Contagions, mené par la chaire d’humanités numériques de l’université de Genève avec le soutien du Fonds national suisse.},
	urldate = {2024-05-15},
	booktitle = {Humanistica 2023},
	publisher = {Association francophone des humanités numériques},
	author = {Carboni, Nicola and Barras, Marie and Joyeux-Prunel, Béatrice},
	month = jun,
	year = {2023},
	keywords = {eda},
	file = {HAL PDF Full Text:/home/clara/Zotero/storage/SNXE9SQG/Carboni et al. - 2023 - Pister des circulations visuelles à l'échelle mond.pdf:application/pdf},
}

@article{joyeux-prunel_oeil_2023,
	title = {Un œil mondial ? {La} mondialisation par l’image au prisme du numérique : le cas du projet {Visual} {Contagions}},
	volume = {55},
	issn = {1262-2966},
	shorttitle = {Un œil mondial ?},
	url = {https://www.cairn.info/revue-societes-et-representations-2023-1-page-203.htm},
	doi = {10.3917/sr.055.0203},
	abstract = {La relation de l’homme à son environnement numérique s’est caractérisée dès l’origine
par un double mouvement de fascination et de répulsion, mis en scène par les artistes
via différents médiums. À partir des années 1990, le numérique a colonisé l’ensemble de
la sphère sociale et en a saturé les espaces, libérant aussi à cette occasion une anxiété
propre à nourrir des fantasmes paranoïaques.
L’attitude de l’homme face au numérique pose alors question : sa prétendue passivité
masque mal, en réalité, la profonde défiance qu’il entretient à son égard. L’acuité de
cette méfiance s’est révélée par l’ampleur des théories complotistes, ainsi celle liée au
vaccin contre le covid, qui aurait permis d’implanter une puce dans le corps de ceux
l’ayant reçu – ce que l’on pourrait appeler un « oeil numérique », digne d’Orwell.
Chercheurs et artistes ne sont pas épargnés, les uns dépendant des sources toujours
plus nombreuses à disposition dans de gigantesques bases de données en ligne, les
autres bousculés par des programmes d’intelligence artificielle qui questionnent leurs
pratiques créatives. C’est que le numérique actualise en fait une question ancienne : où
et comment se forme le regard ?},
	language = {fr},
	number = {1},
	urldate = {2024-05-17},
	journal = {Sociétés \& Représentations},
	author = {Joyeux-Prunel, Béatrice and Carboni, Nicola and Jeanrenaud, Adrien and Viaccoz, Cédric and Belina, Céline and Gauffroy, Thomas and Barras, Marie},
	year = {2023},
	note = {Place: Paris
Publisher: Éditions de la Sorbonne},
	keywords = {eda},
	pages = {203--226},
	file = {Full Text PDF:/home/clara/Zotero/storage/EA4F85LA/Joyeux-Prunel et al. - 2023 - Un œil mondial La mondialisation par l’image au p.pdf:application/pdf;Snapshot:/home/clara/Zotero/storage/GUVIUS4T/revue-societes-et-representations-2023-1-page-203.html:text/html},
}

@inproceedings{caron_emerging_2021,
	title = {Emerging {Properties} in {Self}-{Supervised} {Vision} {Transformers}},
	url = {https://ieeexplore.ieee.org/document/9709990},
	doi = {10.1109/ICCV48922.2021.00951},
	abstract = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) [16] that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder [26], multi-crop training [9], and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
	urldate = {2024-05-18},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jegou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	keywords = {eda},
	pages = {9630--9640},
	file = {Version soumise:/home/clara/Zotero/storage/H8VEVUHG/Caron et al. - 2021 - Emerging Properties in Self-Supervised Vision Tran.pdf:application/pdf},
}

@misc{noauthor_eida_nodate,
	title = {{EIDA}},
	url = {https://imagine.enpc.fr/~kallelis/icdar2024/},
	urldate = {2024-05-23},
	keywords = {eda},
	file = {EIDA:/home/clara/Zotero/storage/D2KLC347/icdar2024.html:text/html},
}

@misc{noauthor_numerisation_nodate,
	title = {La numérisation à la {BnF}},
	url = {https://www.bnf.fr/fr/la-numerisation-la-bnf},
	abstract = {BnF - Site institutionnel},
	language = {fr},
	urldate = {2024-05-25},
	journal = {BnF - Site institutionnel},
	keywords = {eda},
}

@misc{jacquot_decrire_2017,
	type = {Billet},
	title = {Décrire, transcrire et diffuser un corpus documentaire hétérogène : méthodes, formats, outils},
	shorttitle = {Décrire, transcrire et diffuser un corpus documentaire hétérogène},
	url = {https://bnf.hypotheses.org/2214},
	abstract = {Dans le cadre du projet CORPUS, inscrit au plan quadriennal de la recherche 2016-2019 de la Bibliothèque nationale de France, un atelier  intitulé « Décrire, transcrire et diffuser un corpus documentaire hétérogène : méthodes, formats, outils »...},
	language = {fr-FR},
	urldate = {2024-05-25},
	journal = {Carnet de la recherche à la Bibliothèque nationale de France},
	author = {Jacquot, Olivier},
	month = nov,
	year = {2017},
	doi = {10.58079/m3ap},
	note = {ISSN: 2493-4437},
	keywords = {eda},
	file = {Snapshot:/home/clara/Zotero/storage/4GJ8R3E6/2214.html:text/html},
}

@misc{noauthor_about_nodate,
	title = {About — {ALFA} {\textbar} {Digital} {Alfonsine} {Corpus}},
	url = {https://dishas.obspm.fr/about},
	urldate = {2024-05-25},
	keywords = {eda},
	file = {About — ALFA | Digital Alfonsine Corpus:/home/clara/Zotero/storage/TAXE3MUN/about.html:text/html},
}

@article{kermorvant_detection_nodate,
	title = {Détection automatique d’objets dans les images avec {YOLO}},
	language = {fr},
	author = {Kermorvant, Christopher},
	keywords = {eda},
	file = {Kermorvant - Détection automatique d’objets dans les images ave.pdf:/home/clara/Zotero/storage/35HY6NR7/Kermorvant - Détection automatique d’objets dans les images ave.pdf:application/pdf},
}

@misc{noauthor_monde_nodate,
	title = {Le {Monde} en {Sphères}},
	url = {http://expositions.bnf.fr/monde-en-spheres/},
	abstract = {2 500 ans d’histoire de la représentation de la Terre et de l’univers.},
	language = {fr},
	urldate = {2024-05-26},
	journal = {Le Monde en Sphères},
	keywords = {hist astro},
	file = {Snapshot:/home/clara/Zotero/storage/8E35N65S/index.html:text/html},
}

@misc{noauthor_eida_nodate-1,
	title = {{EiDA} {Conference} 2024},
	url = {https://eida.hypotheses.org/conferences/conference-2024},
	abstract = {Graphic Conventions, Visual Idioms, (Dis)Similarity in Astronomical Diagrams Second International EIDA Conference 22–24 April 2024, Observatoire de Paris Organisation: Scott Trigg, Divna Manolova, Samuel Gessner, Matthieu Husson   It will be possible to attend the conference in remotly using this link We encourage in person participation, for this, registration is mandatory by writing an email … Continue reading "Conference 2024"},
	language = {en-US},
	urldate = {2024-05-26},
	journal = {EIDA},
	keywords = {hist astro},
	file = {Snapshot:/home/clara/Zotero/storage/2JGZZ5K9/conference-2024.html:text/html},
}

@article{daston_scientific_2008,
	title = {On {Scientific} {Observation}},
	volume = {99},
	issn = {0021-1753},
	url = {https://www.journals.uchicago.edu/doi/full/10.1086/587535},
	doi = {10.1086/587535},
	abstract = {For much of the last forty years, certain shared epistemological concerns have guided research in both the history and the philosophy of science: the testing of theory (including the replication of experiments), the assessment of evidence, the bearing of theoretical and metaphysical assumptions on the reality of scientific objects, and, above all, the interaction of subjective and objective factors in scientific inquiry. This essay proposes a turn toward ontology—more specifically, toward the ontologies created and sustained by scientific observation. Such a shift in focus would invite a rethinking of the neo‐Kantian distinctions (along with their characteristic metaphors, such as “lenses,” “filters,” and “perspectives”) that have, implicitly or explicitly, informed much of late twentieth‐century history and philosophy of science. In particular, the current gap between psychology and epistemology might be bridged, if the psychology in question were collective rather than individual and the epistemology oriented toward discovery rather than warranting and testing.},
	number = {1},
	urldate = {2024-05-27},
	journal = {Isis},
	author = {Daston, Lorraine},
	month = mar,
	year = {2008},
	note = {Publisher: The University of Chicago Press},
	keywords = {hist astro},
	pages = {97--110},
	file = {Full Text PDF:/home/clara/Zotero/storage/WRYSKTG5/Daston - 2008 - On Scientific Observation.pdf:application/pdf},
}

@article{egiazarian_deep_2020,
	title = {Deep {Vectorization} of {Technical} {Drawings}},
	volume = {12358},
	url = {http://arxiv.org/abs/2003.05471},
	abstract = {We present a new method for vectorization of technical line drawings, such as floor plans, architectural drawings, and 2D CAD images. Our method includes (1) a deep learning-based cleaning stage to eliminate the background and imperfections in the image and fill in missing parts, (2) a transformer-based network to estimate vector primitives, and (3) optimization procedure to obtain the final primitive configurations. We train the networks on synthetic data, renderings of vector line drawings, and manually vectorized scans of line drawings. Our method quantitatively and qualitatively outperforms a number of existing techniques on a collection of representative technical drawings.},
	urldate = {2024-05-27},
	author = {Egiazarian, Vage and Voynov, Oleg and Artemov, Alexey and Volkhonskiy, Denis and Safin, Aleksandr and Taktasheva, Maria and Zorin, Denis and Burnaev, Evgeny},
	year = {2020},
	doi = {10.1007/978-3-030-58601-0_35},
	note = {arXiv:2003.05471 [cs]},
	keywords = {eda},
	pages = {582--598},
	file = {arXiv Fulltext PDF:/home/clara/Zotero/storage/XXRXWEA9/Egiazarian et al. - 2020 - Deep Vectorization of Technical Drawings.pdf:application/pdf;arXiv.org Snapshot:/home/clara/Zotero/storage/D466ALCQ/2003.html:text/html},
}

@misc{ha_neural_2017,
	title = {A {Neural} {Representation} of {Sketch} {Drawings}},
	url = {http://arxiv.org/abs/1704.03477},
	doi = {10.48550/arXiv.1704.03477},
	abstract = {We present sketch-rnn, a recurrent neural network (RNN) able to construct stroke-based drawings of common objects. The model is trained on thousands of crude human-drawn images representing hundreds of classes. We outline a framework for conditional and unconditional sketch generation, and describe new robust training methods for generating coherent sketch drawings in a vector format.},
	urldate = {2024-05-27},
	publisher = {arXiv},
	author = {Ha, David and Eck, Douglas},
	month = may,
	year = {2017},
	note = {arXiv:1704.03477 [cs, stat]},
	keywords = {eda},
	file = {arXiv Fulltext PDF:/home/clara/Zotero/storage/FGPJGV3H/Ha et Eck - 2017 - A Neural Representation of Sketch Drawings.pdf:application/pdf;arXiv.org Snapshot:/home/clara/Zotero/storage/E3DU7RPT/1704.html:text/html},
}

@article{hilaire_robust_2006,
	title = {Robust and accurate vectorization of line drawings},
	volume = {28},
	issn = {1939-3539},
	url = {https://ieeexplore.ieee.org/document/1624354},
	doi = {10.1109/TPAMI.2006.127},
	abstract = {This paper presents a method for vectorizing the graphical parts of paper-based line drawings. The method consists of separating the input binary image into layers of homogeneous thickness, skeletonizing each layer, segmenting the skeleton by a method based on random sampling, and simplifying the result. The segmentation method is robust with a best bound of 50 percent noise reached for indefinitely long primitives. Accurate estimation of the recognized vector's parameters is enabled by explicitly computing their feasibility domains. Theoretical performance analysis and expression of the complexity of the segmentation method are derived. Experimental results and comparisons with other vectorization systems are also provided.},
	number = {6},
	urldate = {2024-05-27},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Hilaire, X. and Tombre, K.},
	month = jun,
	year = {2006},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {eda},
	pages = {890--904},
	file = {IEEE Xplore Abstract Record:/home/clara/Zotero/storage/J4G5SWX8/1624354.html:text/html;Version soumise:/home/clara/Zotero/storage/93RXW8SG/Hilaire et Tombre - 2006 - Robust and accurate vectorization of line drawings.pdf:application/pdf},
}

@article{obrist_visual_2012,
	title = {Visual {Representation} and {Science}: {Visual} {Figures} of the {Universe} between {Antiquity} and the {Early} {Thirteenth} {Century}},
	volume = {6},
	copyright = {Copyright (c)},
	issn = {1913-0465},
	shorttitle = {Visual {Representation} and {Science}},
	url = {https://spontaneousgenerations.library.utoronto.ca/index.php/SpontaneousGenerations/article/view/17158},
	doi = {10.4245/sponge.v6i1.17158},
	abstract = {The paper raises the question of the function of visual representations in medieval cosmographical texts. It proposes to view diverse functions of figures in relation to changing discursive environments, including differing philosophical positions and changing social and intellectual contexts. It further suggests a distinction between figures that were elaborated within the highly specialized disciplines of mathematics and philosophy of nature in Greek Antiquity and figures that were instrumental in transmitting accepted world models, thus avoiding the opposition between scientific and unscientific types of verbal and pictorial documents. Simplifying changes, when figures are abstracted form their geometrical context and accompany doxographical, descriptive accounts, are characterized in terms of schematization. Concomitantly, mathematical and philosophical demonstrations tend to give way to proofs of a predominantly rhetorical nature: images are verbally construed and, in order to enhance these, actual visual figures— mostly linear, diagrammatic constructs—are added. With regard to the Middle Ages, the paper distinguishes two principal periods: the period from the seventh to the eleventh century and the period of the so-called twelfth-century Renaissance. First, the verbal and pictorial cosmological corpus of Roman origin gave rise to explanations and variations but not to consequential theoretical developments and cosmological diagrams tended to fuse with summarizing tables at this time. Then, during the twelfth century, mathematical and philosophical documents of a specialized kind that were translated from the Arabic and also from the Greek became available in the Latin West. In mathematics, specialized types of study remained, however, sparse. Continuous elaborations of the assimilated material set in later only, within the thirteenth-century university context. Nevertheless, twelfth-century authors of cosmographical accounts became increasingly aware that their expositions and visual figures were ultimately derived from geometrical models of the universe. More diversified types of demonstration and corresponding visual figures were being used, as exemplified by William of Conches’ Dragmaticon philosophiae.},
	language = {en},
	number = {1},
	urldate = {2024-05-27},
	journal = {Spontaneous Generations: A Journal for the History and Philosophy of Science},
	author = {Obrist, Barbara},
	month = oct,
	year = {2012},
	note = {Number: 1},
	keywords = {hist astro},
	pages = {15--23},
	file = {Full Text PDF:/home/clara/Zotero/storage/YWW39VQ5/Obrist - 2012 - Visual Representation and Science Visual Figures .pdf:application/pdf},
}

@book{kupfer_visualization_2020,
	title = {The {Visualization} of {Knowledge} in {Medieval} and {Early} {Modern} {Europe} - {Marcia} {Kupfer},{Adam} {S}. {Cohen},{Jeffrey} {Howard} {Chajes}},
	isbn = {978-2-503-58303-7},
	url = {https://www.decitre.fr/livres/the-visualization-of-knowledge-in-medieval-and-early-modern-europe-9782503583037.html},
	abstract = {All of us are exposed to graphic means of communication on a daily basis. Our life seems flooded with lists, tables, charts, diagrams, models, maps, and forms of notation. Although we now take such devices for granted, their role in the codification and transmission of knowledge evolved within h...},
	language = {en},
	urldate = {2024-05-27},
	publisher = {Brepols},
	author = {Kupfer, Marcia and Cohen, Adam S. and Chajes, Howard},
	year = {2020},
	keywords = {hist astro},
	file = {Snapshot:/home/clara/Zotero/storage/Z6QG4SW6/the-visualization-of-knowledge-in-medieval-and-early-modern-europe-9782503583037.html:text/html},
}

@misc{malkov_efficient_2018,
	title = {Efficient and robust approximate nearest neighbor search using {Hierarchical} {Navigable} {Small} {World} graphs},
	url = {http://arxiv.org/abs/1603.09320},
	doi = {10.48550/arXiv.1603.09320},
	abstract = {We present a new approach for the approximate K-nearest neighbor search based on navigable small world graphs with controllable hierarchy (Hierarchical NSW, HNSW). The proposed solution is fully graph-based, without any need for additional search structures, which are typically used at the coarse search stage of the most proximity graph techniques. Hierarchical NSW incrementally builds a multi-layer structure consisting from hierarchical set of proximity graphs (layers) for nested subsets of the stored elements. The maximum layer in which an element is present is selected randomly with an exponentially decaying probability distribution. This allows producing graphs similar to the previously studied Navigable Small World (NSW) structures while additionally having the links separated by their characteristic distance scales. Starting search from the upper layer together with utilizing the scale separation boosts the performance compared to NSW and allows a logarithmic complexity scaling. Additional employment of a heuristic for selecting proximity graph neighbors significantly increases performance at high recall and in case of highly clustered data. Performance evaluation has demonstrated that the proposed general metric space search index is able to strongly outperform previous opensource state-of-the-art vector-only approaches. Similarity of the algorithm to the skip list structure allows straightforward balanced distributed implementation.},
	urldate = {2024-05-29},
	publisher = {arXiv},
	author = {Malkov, Yu A. and Yashunin, D. A.},
	month = aug,
	year = {2018},
	note = {arXiv:1603.09320 [cs]},
	keywords = {eda},
	annote = {Comment: 13 pages, 15 figures},
	file = {arXiv Fulltext PDF:/home/clara/Zotero/storage/B65NUTB2/Malkov et Yashunin - 2018 - Efficient and robust approximate nearest neighbor .pdf:application/pdf;arXiv.org Snapshot:/home/clara/Zotero/storage/4QCC6BTC/1603.html:text/html},
}

@inproceedings{janes_towards_2021,
	address = {Paris, France},
	title = {Towards automatic {TEI} encoding via layout analysis},
	url = {https://hal.science/hal-03527287},
	urldate = {2024-06-06},
	booktitle = {Fantastic future 21, 3rd {International} {Conference} on {Artificial} {Intelligence} for {Librairies}, {Archives} and {Museums}},
	publisher = {AI for Libraries, Archives, and Museums (ai4lam)},
	author = {Janes, Juliette and Pinche, Ariane and Jahan, Claire and Gabay, Simon},
	month = dec,
	year = {2021},
	keywords = {eda},
	file = {HAL PDF Full Text:/home/clara/Zotero/storage/YIPEKUKY/Janes et al. - 2021 - Towards automatic TEI encoding via layout analysis.pdf:application/pdf},
}

@inproceedings{sagot_gallicorpor_2022,
	address = {Paris, France},
	title = {Gallic(orpor)a : {Extraction}, annotation et diffusion de l'information textuelle et visuelle en diachronie longue},
	shorttitle = {Gallic(orpor)a},
	url = {https://hal.science/hal-03930542},
	urldate = {2024-06-06},
	booktitle = {{DataLab} de la {BnF} : {Restitution} des travaux 2022},
	publisher = {DataLab de la BnF},
	author = {Sagot, Benoît and Romary, Laurent and Bawden, Rachel and Ortiz Suárez, Pedro Javier and Christensen, Kelly and Gabay, Simon and Pinche, Ariane and Camps, Jean-Baptiste},
	month = dec,
	year = {2022},
	keywords = {eda},
	file = {HAL PDF Full Text:/home/clara/Zotero/storage/6BYHXKIX/Sagot et al. - 2022 - Gallic(orpor)a  Extraction, annotation et diffusi.pdf:application/pdf},
}

@inproceedings{gabay_standardizing_2020,
	address = {Hammamet, Tunisia},
	title = {Standardizing linguistic data: method and tools for annotating (pre-orthographic) {French}},
	shorttitle = {Standardizing linguistic data},
	url = {https://hal.science/hal-03018381},
	doi = {10.1145/3423603.3423996},
	abstract = {With the development of big corpora of various periods, it becomes crucial to standardise linguistic annotation (e.g. lemmas, POS tags, morphological annotation) to increase the interoperability of the data produced, despite diachronic variations. In the present paper, we describe both methodologically (by proposing annotation principles) and technically (by creating the required training data and the relevant models) the production of a linguistic tagger for (early) modern French (16-18th c.), taking as much as possible into account already existing standards for contemporary and, especially, medieval French.},
	urldate = {2024-06-10},
	booktitle = {Proceedings of the 2nd {International} {Digital} {Tools} \& {Uses} {Congress} ({DTUC} '20)},
	author = {Gabay, Simon and Clérice, Thibault and Camps, Jean-Baptiste and Tanguy, Jean-Baptiste and Gille-Levenson, Matthias},
	month = oct,
	year = {2020},
	keywords = {eda},
	file = {HAL PDF Full Text:/home/clara/Zotero/storage/ZRS3XR3F/Gabay et al. - 2020 - Standardizing linguistic data method and tools fo.pdf:application/pdf},
}

@misc{noauthor_escriptorium_nodate,
	title = {{eScriptorium} - {Homepage}},
	url = {https://test2.fondue.unige.ch/},
	urldate = {2024-06-15},
	keywords = {eda},
	file = {eScriptorium - Homepage:/home/clara/Zotero/storage/VNPDX9DK/test2.fondue.unige.ch.html:text/html},
}

@misc{noauthor_resilience_nodate,
	title = {{RESILIENCE} {Tool}: {eScriptorium}},
	shorttitle = {{RESILIENCE} {Tool}},
	url = {https://www.resilience-ri.eu/blog/resilience-tool-escriptorium/},
	abstract = {EPHE The Digital Humanities team at the École Pratique des Hautes Études (EPHE) – University PSL has been developing this cutting-edge Deep Learning software for automatically reading and transcribing documents in many different scripts and languages. eScriptorium The purpose of eScriptorium is to provide as complete as possible a workflow for the production of digital […]},
	urldate = {2024-06-15},
	journal = {RESILIENCE},
	keywords = {eda},
	file = {Snapshot:/home/clara/Zotero/storage/FS3NV9C4/resilience-tool-escriptorium.html:text/html},
}

@misc{noauthor_imagenet_nodate,
	title = {{ImageNet}},
	url = {https://www.image-net.org/},
	urldate = {2024-06-20},
	keywords = {eda},
	file = {ImageNet:/home/clara/Zotero/storage/BV8QSTPR/www.image-net.org.html:text/html},
}

@misc{noauthor_coco_nodate,
	title = {{COCO} - {Common} {Objects} in {Context}},
	url = {https://cocodataset.org/#home},
	urldate = {2024-06-20},
	keywords = {eda},
	file = {COCO - Common Objects in Context:/home/clara/Zotero/storage/TYH3E8UQ/cocodataset.org.html:text/html},
}

@inproceedings{monnier_docextractor_2020,
	title = {{docExtractor}: {An} off-the-shelf historical document element extraction},
	shorttitle = {{docExtractor}},
	url = {http://arxiv.org/abs/2012.08191},
	doi = {10.1109/ICFHR2020.2020.00027},
	abstract = {We present docExtractor, a generic approach for extracting visual elements such as text lines or illustrations from historical documents without requiring any real data annotation. We demonstrate it provides high-quality performances as an off-the-shelf system across a wide variety of datasets and leads to results on par with state-of-the-art when fine-tuned. We argue that the performance obtained without fine-tuning on a specific dataset is critical for applications, in particular in digital humanities, and that the line-level page segmentation we address is the most relevant for a general purpose element extraction engine. We rely on a fast generator of rich synthetic documents and design a fully convolutional network, which we show to generalize better than a detection-based approach. Furthermore, we introduce a new public dataset dubbed IlluHisDoc dedicated to the fine evaluation of illustration segmentation in historical documents.},
	urldate = {2024-06-20},
	booktitle = {2020 17th {International} {Conference} on {Frontiers} in {Handwriting} {Recognition} ({ICFHR})},
	author = {Monnier, Tom and Aubry, Mathieu},
	month = sep,
	year = {2020},
	note = {arXiv:2012.08191 [cs]},
	keywords = {eda},
	pages = {91--96},
	annote = {Comment: Accepted at 2020 17th International Conference on Frontiers in Handwriting Recognition (ICFHR) (oral). Project webpage: http://imagine.enpc.fr/{\textasciitilde}monniert/docExtractor/},
	file = {arXiv Fulltext PDF:/home/clara/Zotero/storage/G9BARRAA/Monnier et Aubry - 2020 - docExtractor An off-the-shelf historical document.pdf:application/pdf;arXiv.org Snapshot:/home/clara/Zotero/storage/G5AH9V5W/2012.html:text/html},
}

@misc{kalleli_historical_2024,
	title = {Historical {Astronomical} {Diagrams} {Decomposition} in {Geometric} {Primitives}},
	url = {http://arxiv.org/abs/2403.08721},
	doi = {10.48550/arXiv.2403.08721},
	abstract = {Automatically extracting the geometric content from the hundreds of thousands of diagrams drawn in historical manuscripts would enable historians to study the diffusion of astronomical knowledge on a global scale. However, state-of-the-art vectorization methods, often designed to tackle modern data, are not adapted to the complexity and diversity of historical astronomical diagrams. Our contribution is thus twofold. First, we introduce a unique dataset of 303 astronomical diagrams from diverse traditions, ranging from the XIIth to the XVIIIth century, annotated with more than 3000 line segments, circles and arcs. Second, we develop a model that builds on DINO-DETR to enable the prediction of multiple geometric primitives. We show that it can be trained solely on synthetic data and accurately predict primitives on our challenging dataset. Our approach widely improves over the LETR baseline, which is restricted to lines, by introducing a meaningful parametrization for multiple primitives, jointly training for detection and parameter refinement, using deformable attention and training on rich synthetic data. Our dataset and code are available on our webpage.},
	urldate = {2024-06-26},
	publisher = {arXiv},
	author = {Kalleli, Syrine and Trigg, Scott and Albouy, Ségolène and Husson, Mathieu and Aubry, Mathieu},
	month = mar,
	year = {2024},
	note = {arXiv:2403.08721 [cs]},
	keywords = {eda},
	annote = {Comment: Code and dataset are available in http://imagine.enpc.fr/{\textasciitilde}kallelis/icdar2024/},
	file = {arXiv Fulltext PDF:/home/clara/Zotero/storage/8UIE7GXM/Kalleli et al. - 2024 - Historical Astronomical Diagrams Decomposition in .pdf:application/pdf;arXiv.org Snapshot:/home/clara/Zotero/storage/DUTSHHM4/2403.html:text/html},
}

@article{indolia_conceptual_2018,
	series = {International {Conference} on {Computational} {Intelligence} and {Data} {Science}},
	title = {Conceptual {Understanding} of {Convolutional} {Neural} {Network}- {A} {Deep} {Learning} {Approach}},
	volume = {132},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050918308019},
	doi = {10.1016/j.procs.2018.05.069},
	abstract = {Deep learning has become an area of interest to the researchers in the past few years. Convolutional Neural Network (CNN) is a deep learning approach that is widely used for solving complex problems. It overcomes the limitations of traditional machine learning approaches. The motivation of this study is to provide the knowledge and understanding about various aspects of CNN. This study provides the conceptual understanding of CNN along with its three most common architectures, and learning algorithms. This study will help researchers to have a broad comprehension of CNN and motivate them to venture in this field. This study will be a resource and quick reference for those who are interested in this field.},
	urldate = {2024-06-28},
	journal = {Procedia Computer Science},
	author = {Indolia, Sakshi and Goswami, Anil Kumar and Mishra, S. P. and Asopa, Pooja},
	month = jan,
	year = {2018},
	keywords = {ia},
	pages = {679--688},
	file = {ScienceDirect Snapshot:/home/clara/Zotero/storage/A4AS9I5T/S1877050918308019.html:text/html},
}

@misc{redmon_you_2016,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	shorttitle = {You {Only} {Look} {Once}},
	url = {http://arxiv.org/abs/1506.02640},
	doi = {10.48550/arXiv.1506.02640},
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
	urldate = {2024-06-28},
	publisher = {arXiv},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	month = may,
	year = {2016},
	note = {arXiv:1506.02640 [cs]},
	keywords = {eda},
	file = {arXiv Fulltext PDF:/home/clara/Zotero/storage/F4B3MIWN/Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf:application/pdf;arXiv.org Snapshot:/home/clara/Zotero/storage/XK3MFX94/1506.html:text/html},
}

@misc{lin_microsoft_2015,
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	shorttitle = {Microsoft {COCO}},
	url = {http://arxiv.org/abs/1405.0312},
	doi = {10.48550/arXiv.1405.0312},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	urldate = {2024-07-01},
	publisher = {arXiv},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
	month = feb,
	year = {2015},
	note = {arXiv:1405.0312 [cs]},
	keywords = {eda},
	annote = {Comment: 1) updated annotation pipeline description and figures; 2) added new section describing datasets splits; 3) updated author list},
	file = {arXiv Fulltext PDF:/home/clara/Zotero/storage/SVTYDY5P/Lin et al. - 2015 - Microsoft COCO Common Objects in Context.pdf:application/pdf;arXiv.org Snapshot:/home/clara/Zotero/storage/T7EHAFM8/1405.html:text/html},
}

@inproceedings{roughan_digital_2014,
	address = {New York, NY, USA},
	series = {{DATeCH} '14},
	title = {Digital editions and diplomatic diagrams},
	isbn = {978-1-4503-2588-2},
	url = {https://doi.org/10.1145/2595188.2595189},
	doi = {10.1145/2595188.2595189},
	abstract = {In this paper, the Archimedes Project at the College of the Holy Cross presents an approach to digital, diplomatic renditions of Greek mathematical diagrams as preserved in ancient and medieval sources. The team creates XML-based vector images from scalable vector graphics (SVGs). The resulting tracings are composed of data on the geometric shapes that make up the diagram, the textual labels that accompany them, citable identifiers, and editorial annotations. The relationships between the diplomatic diagram and the diplomatic text and images of the physical manuscript are modeled with the CITE architecture developed by the Homer Multitext project (http://www.homermultitext.org/hmt-doc/cite/index.html).This approach creates machine-actionable data out of raster images provided by digital photography. The team shows how they have applied this method to diagrams from Codex Bodmer 8 (16th century) and Codex C (10th century and preserved in the Archimedes Palimpsest).},
	booktitle = {Proceedings of the {First} {International} {Conference} on {Digital} {Access} to {Textual} {Cultural} {Heritage}},
	publisher = {Association for Computing Machinery},
	author = {Roughan, Christine},
	year = {2014},
	note = {event-place: Madrid, Spain},
	keywords = {edit},
	pages = {77--82},
}

@misc{juneja_deep_2023,
	title = {Deep {Learning} vs {Machine} {Learning} — {The} {Difference} {Explained}!},
	url = {https://www.dataquest.io/blog/deep-learning-vs-machine-learning-the-difference-explained/},
	abstract = {Machine learning is a subset of AI a computer system uses to make predictions or decisions. Deep Learning is a subset of ML that uses artificial neural networks to solve more complex problems.},
	language = {en-US},
	urldate = {2024-07-05},
	journal = {Dataquest},
	author = {Juneja, Sahil},
	month = mar,
	year = {2023},
	keywords = {ia},
	file = {Snapshot:/home/clara/Zotero/storage/A4M4YLET/deep-learning-vs-machine-learning-the-difference-explained.html:text/html},
}

@misc{noauthor_what_nodate,
	title = {What {Is} {Deep} {Learning} and {How} {Does} {It} {Work}?},
	url = {https://builtin.com/machine-learning/deep-learning},
	abstract = {Deep learning uses multi-layered structures of algorithms called neural networks to draw similar conclusions as humans would. Here’s how it works.},
	language = {en},
	urldate = {2024-07-05},
	journal = {Built In},
	keywords = {ia},
	file = {Snapshot:/home/clara/Zotero/storage/VFQV6CIX/deep-learning.html:text/html},
}

@misc{noauthor_about_nodate-1,
	title = {About the {CITE} architecture · {The} {CITE} {Architecture}},
	url = {https://cite-architecture.github.io/about/},
	urldate = {2024-07-09},
	keywords = {edit},
	file = {About the CITE architecture · The CITE Architecture:/home/clara/Zotero/storage/CKN5MDDZ/about.html:text/html},
}

@incollection{blackwell_cite_2019,
	title = {The {CITE} {Architecture}: a {Conceptual} and {Practical} {Overview}},
	copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
	isbn = {978-3-11-059957-2},
	shorttitle = {The {CITE} {Architecture}},
	url = {https://www.degruyter.com/document/doi/10.1515/9783110599572-006/html?lang=de},
	abstract = {Das Kapitel The CITE Architecture: a Conceptual and Practical Overview erschien in Digital Classical Philology auf Seite 73.},
	language = {en},
	urldate = {2024-07-09},
	booktitle = {The {CITE} {Architecture}: a {Conceptual} and {Practical} {Overview}},
	publisher = {De Gruyter Saur},
	author = {Blackwell, Christopher W. and Smith, Neel},
	month = aug,
	year = {2019},
	doi = {10.1515/9783110599572-006},
	pages = {73--94},
	keywords = {edit},
	file = {Full Text PDF:/home/clara/Zotero/storage/JT2EMVIJ/Blackwell et Smith - 2019 - The CITE Architecture a Conceptual and Practical .pdf:application/pdf},
}

@misc{noauthor_homer_nodate,
	title = {The {Homer} {Multitext} project},
	url = {https://www.homermultitext.org/},
	urldate = {2024-07-09},
	keywords = {edit},
	file = {The Homer Multitext project:/home/clara/Zotero/storage/QKFBS3AW/www.homermultitext.org.html:text/html},
}

@misc{noauthor_leibniz_nodate,
	title = {Leibniz {Online} - {Leibniz}-{Edition} {Berlin}},
	url = {https://leibniz-berlin.bbaw.de/de/leibniz-online},
	urldate = {2024-07-10},
	keywords = {edit},
	file = {Leibniz Online - Leibniz-Edition Berlin:/home/clara/Zotero/storage/JJDRKSXC/leibniz-online.html:text/html},
}

@misc{noauthor_telotaleibnizviii-latex_tei_2023,
	title = {telota/{LeibnizVIII}-{LaTeX}\_TEI},
	url = {https://github.com/telota/LeibnizVIII-LaTeX_TEI},
	urldate = {2024-07-10},
	publisher = {TELOTA - The Electronic Life Of The Academy},
	month = mar,
	year = {2023},
	keywords = {edit},
	note = {original-date: 2021-02-24T13:06:06Z},
}

@incollection{clavert_lhistorien_2012,
	address = {Paris},
	series = {La {Non}-{Collection}},
	title = {L’historien programmeur ?},
	copyright = {https://creativecommons.org/licenses/by-sa/4.0/},
	isbn = {978-2-7351-1527-3},
	shorttitle = {L’historien programmeur ?},
	url = {https://books.openedition.org/editionsmsh/305},
	abstract = {À partir du projet de site internet et de livre qui sont devenus collaboratifs pour la préparation d’une deuxième édition - et sur la base d’exemples concrets, cet atelier se propose de discuter de l’intérêt, pour des chercheurs en sciences humaines et sociales, d’apprendre les bases de la programmation.},
	language = {fr},
	urldate = {2024-07-16},
	booktitle = {{THATCamp} {Paris} 2012 : {Non}-actes de la non-conférence des humanités numériques},
	publisher = {Éditions de la Maison des sciences de l’homme},
	author = {Clavert, Frédéric and Berra, Aurelien and Heimburger, Franziska},
	year = {2012},
	doi = {10.4000/books.editionsmsh.305},
	note = {Code: THATCamp Paris 2012 : Non-actes de la non-conférence des humanités numériques},
	keywords = {ia},
	file = {Texte intégral:/home/clara/Zotero/storage/3TFE65GV/Collectif - 2012 - L’historien programmeur   Proposé par  Frédéric.pdf:application/pdf},
}

@book{trovato_everything_2014,
	title = {Everything {You} {Always} {Wanted} to {Know} about {Lachmann}'s {Method}},
	isbn = {978-88-6292-528-0},
	abstract = {"This book, written mainly with the non-Italian reader in mind, addresses a central problem in textual criticism...namely, how to try to correctly reconstruct a text of the past so that, even if not identical, it is as close as possible to the lost original, starting from a number of copies more or less full of mistakes; that is to say, how to preserve part of the memory of our past."--Preface, p. [13].},
	language = {en},
	publisher = {libreriauniversitaria.it Edizioni},
	author = {Trovato, Paolo},
	year = {2014},
	note = {Google-Books-ID: VZqlBAAAQBAJ},
	keywords = {edit},
}

@misc{michez_gallicapix_2021,
	title = {{GallicaPix}, un nouvel outil d’exploration iconographique},
	url = {https://gallica.bnf.fr/blog/21062021/gallicapix-un-nouvel-outil-dexploration-iconographique?mode=desktop},
	urldate = {2024-07-29},
	journal = {Le blog de Gallica},
	author = {Michez, Guillaume},
	month = jun,
	year = {2021},
	keywords = {eda},
	file = {GallicaPix, un nouvel outil d’exploration iconographique | Le blog de Gallica:/home/clara/Zotero/storage/N5L6X7ZB/gallicapix-un-nouvel-outil-dexploration-iconographique.html:text/html},
}

@techreport{beaudouin_cartographie_2017,
	type = {Research {Report}},
	title = {Cartographie de la {Grande} {Guerre} sur le {Web}},
	url = {https://hal.science/hal-01425600},
	abstract = {Les sources documentaires numérisées autour de la guerre 14-18 offrent un terrain particulièrement favorable à l’observation des usages du patrimoine en ligne, du fait de leur diversité et de l’intensité des activités qu’elles suscitent sur le web. Par ce projet de recherche, il s’agissait de mieux comprendre la manière dont ces sources circulent, sont enrichies, voire transformées sur le web. À travers, mais également au-delà de l’objet singulier du patrimoine 14-18, ce programme de recherche a permis d’éprouver des outils d’analyse du web rigoureux et de dégager des axes forts de réflexion pour la valorisation du patrimoine en ligne.
La deuxième phase de ce projet a consisté en une cartographie du web français de la Grande Guerre, à partir de la collecte d’environ 500 sites réalisée dans le cadre du dépôt légal d’internet ; puis d'une analyse quantitative du forum Pages 14-18, lieu central des échanges en ligne sur la Grande Guerre, afin de décrire la manière dont les documents y sont cités et partagés entre ses membres. Cette phase fut complétée par 12 entretiens approfondis avec des acteurs du web de la Grande Guerre.},
	urldate = {2024-07-29},
	institution = {Bibliothèque nationale de France ; Bibliothèque de documentation internationale contemporaine ; Télécom ParisTech},
	author = {Beaudouin, Valérie and Pehlivan, Zeynep},
	month = jan,
	year = {2017},
	keywords = {eda},
	file = {HAL PDF Full Text:/home/clara/Zotero/storage/BB4DRHA8/Beaudouin et Pehlivan - 2017 - Cartographie de la Grande Guerre sur le Web.pdf:application/pdf},
}

@misc{noauthor_tutoriel_2024,
	title = {Tutoriel {SVG} - {SVG} ({Scalable} {Vector} {Graphics}) {\textbar} {MDN}},
	url = {https://developer.mozilla.org/fr/docs/Web/SVG/Tutorial},
	abstract = {SVG, pour Scalable Vector Graphics (ou encore Graphismes vectoriels redimensionnables), est un langage basé sur le XML du W3C qui permet de définir des éléments graphiques avec des balises. Ce langage est plus ou moins implémenté dans Firefox, Opera, les navigateurs à base de Webkit, Internet Explorer et les autres navigateurs Web.},
	language = {fr},
	urldate = {2024-08-03},
	month = jul,
	year = {2024},
	keywords = {edit},
	file = {Snapshot:/home/clara/Zotero/storage/K45MB4DF/Tutorial.html:text/html},
}

@misc{noauthor_reference_2024,
	title = {Référence des attributs {SVG} - {SVG} ({Scalable} {Vector} {Graphics}) {\textbar} {MDN}},
	url = {https://developer.mozilla.org/fr/docs/Web/SVG/Attribute},
	abstract = {Les éléments SVG peuvent être modifiés en utilisant des attributs qui spécifient comment les éléments doivent être traités ou présentés.},
	language = {fr},
	urldate = {2024-08-03},
	month = jul,
	year = {2024},
	keywords = {edit},
	file = {Snapshot:/home/clara/Zotero/storage/KGTPF9ZR/Attribute.html:text/html},
}

@misc{mallory_iiif_2019,
	title = {{IIIF} for museums, explained},
	url = {https://blog.cogapp.com/iiif-for-museums-explained-49fd0560e1ba},
	abstract = {What IIIF is; who’s using it; and why you should consider it},
	language = {en},
	urldate = {2024-08-07},
	journal = {Medium},
	author = {Mallory, Gavin},
	month = jul,
	year = {2019},
	keywords = {ia},
	file = {Snapshot:/home/clara/Zotero/storage/T9M35AFN/iiif-for-museums-explained-49fd0560e1ba.html:text/html},
}

@misc{clavert_2dh_2015,
	type = {Billet},
	title = {{2DH} — {Design} \& {Digital} {Humanities} : le design comme méthode pour les {Humanités} {Numériques} ?},
	shorttitle = {{2DH} — {Design} \& {Digital} {Humanities}},
	url = {https://tcp.hypotheses.org/849},
	abstract = {Proposition d’atelier pour THATCamp Paris 2015 (9-­11 juin) par Stéphane Vial (Université de Nîmes, UMR ACTE 8218) et Yves Rinato (INTACTILE Design, U. de Nîmes) Présentation Au sens large, les Digital Humanities ou Humanités...},
	language = {fr-FR},
	urldate = {2024-08-08},
	journal = {THATCamp Paris},
	author = {Clavert, Frédéric},
	month = may,
	year = {2015},
	doi = {10.58079/uo2u},
	note = {ISSN: 2261-2971},
	keywords = {ia},
	file = {Snapshot:/home/clara/Zotero/storage/ATG5YVSH/849.html:text/html},
}

@misc{husson_eida_2022,
	title = {{EiDA} - funding proposal},
	author = {Husson, Mathieu},
	keywords = {eda},
	year = {2022},
}

@misc{bouchard_presentation_2017,
	type = {Billet},
	title = {Présentation du projet {CORPUS} à la {BnF}},
	url = {https://webcorpora.hypotheses.org/119},
	abstract = {CORPUS est un programme de recherche visant à préfigurer “un service de fourniture de corpus numériques à destination de la recherche“. Concrètement, il s’agit de fournir à des chercheurs des données et des outils...},
	language = {fr-FR},
	urldate = {2024-08-09},
	journal = {Web Corpora},
	author = {Bouchard, Ariane},
	month = may,
	year = {2017},
	doi = {10.58079/va8e},
	note = {ISSN: 2608-1393},
	keywords = {eda},
	file = {Snapshot:/home/clara/Zotero/storage/HFIAZ5G4/119.html:text/html},
}

@inproceedings{chague_htr-united_2021,
	address = {Lille, France},
	title = {{HTR}-{United} : {Mutualisons} la vérité de terrain !},
	shorttitle = {{HTR}-{United}},
	url = {https://hal.science/hal-03398740},
	urldate = {2024-08-09},
	booktitle = {{DHNord2021} - {Publier}, partager, réutiliser les données de la recherche : les data papers et leurs enjeux},
	publisher = {MESHS},
	author = {Chagué, Alix and Clérice, Thibault and Romary, Laurent},
	month = nov,
	year = {2021},
	keywords = {eda},
	file = {HAL PDF Full Text:/home/clara/Zotero/storage/HUE75CH8/Chagué et al. - 2021 - HTR-United  Mutualisons la vérité de terrain !.pdf:application/pdf},
}

@article{moureau_apparatus_2015,
	title = {The {Apparatus} {Criticus}},
	url = {https://dial.uclouvain.be/pr/boreal/object/boreal:155652},
	language = {fr},
	urldate = {2024-08-14},
	author = {Moureau, Sébastien},
	year = {2015},
	keywords = {edit},
	file = {Full Text PDF:/home/clara/Zotero/storage/E6DLTCF7/Moureau - 2015 - The Apparatus Criticus.pdf:application/pdf},
}

@misc{noauthor_ransac_nodate,
	title = {{RANSAC}},
	url = {https://www.mathworks.com/discovery/ransac.html},
	abstract = {Learn about the applications of RANSAC in computer vision using MATLAB and Simulink. Resources include video, examples, source code, and technical documentation.},
	language = {en},
	urldate = {2024-08-14},
	keywords = {eda},
	file = {Snapshot:/home/clara/Zotero/storage/LDCXGJYY/ransac.html:text/html},
}

@inproceedings{lee_newspaper_2020,
	address = {Virtual Event Ireland},
	title = {The {Newspaper} {Navigator} {Dataset}: {Extracting} {Headlines} and {Visual} {Content} from 16 {Million} {Historic} {Newspaper} {Pages} in {Chronicling} {America}},
	isbn = {978-1-4503-6859-9},
	shorttitle = {The {Newspaper} {Navigator} {Dataset}},
	url = {https://dl.acm.org/doi/10.1145/3340531.3412767},
	doi = {10.1145/3340531.3412767},
	language = {en},
	urldate = {2024-08-14},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Information} \& {Knowledge} {Management}},
	publisher = {ACM},
	author = {Lee, Benjamin Charles Germain and Mears, Jaime and Jakeway, Eileen and Ferriter, Meghan and Adams, Chris and Yarasavage, Nathan and Thomas, Deborah and Zwaard, Kate and Weld, Daniel S.},
	month = oct,
	year = {2020},
	pages = {3055--3062},
	keywords = {eda},
	file = {Texte intégral:/home/clara/Zotero/storage/NVMP7UKD/Lee et al. - 2020 - The Newspaper Navigator Dataset Extracting Headli.pdf:application/pdf},
}

@misc{noauthor_sphere_nodate,
	title = {The {Sphere}},
	url = {https://sphaera.mpiwg-berlin.mpg.de/},
	urldate = {2024-08-14},
	keywords = {eda},
	file = {The Sphere:/home/clara/Zotero/storage/AC5XKIIS/sphaera.mpiwg-berlin.mpg.de.html:text/html},
}

@misc{noauthor_horae_nodate,
	title = {{HORAE} - {Hours}: {Recognition}, {Analysis}, {Editions}},
	url = {https://heurist.huma-num.fr/heurist/?db=stutzmann_horae&website&id=378891},
	urldate = {2024-08-14},
	keywords = {eda},
	file = {HORAE - Hours\: Recognition, Analysis, Editions:/home/clara/Zotero/storage/AJIBJBDA/heurist.html:text/html},
}

@misc{noauthor_wordnet_nodate,
	title = {{WordNet}},
	url = {https://wordnet.princeton.edu/homepage},
	abstract = {What is WordNet? Any opinions, findings, and conclusions or recommendations expressed in this material are those of the creators of WordNet and do not necessarily reflect the views of any funding agency or Princeton University. When writing a paper or producing a software application, tool, or interface based on WordNet, it is necessary to prope...},
	language = {en},
	urldate = {2024-08-14},
	journal = {WordNet},
	keywords = {eda},
	file = {Snapshot:/home/clara/Zotero/storage/6D7WU7RW/wordnet.princeton.edu.html:text/html},
}

@misc{he_momentum_2020,
	title = {Momentum {Contrast} for {Unsupervised} {Visual} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1911.05722},
	doi = {10.48550/arXiv.1911.05722},
	abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
	urldate = {2024-08-14},
	publisher = {arXiv},
	author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
	month = mar,
	year = {2020},
	note = {arXiv:1911.05722 [cs]},
	keywords = {eda},
	annote = {Comment: CVPR 2020 camera-ready. Code: https://github.com/facebookresearch/moco},
	file = {arXiv Fulltext PDF:/home/clara/Zotero/storage/XPAWQ54T/He et al. - 2020 - Momentum Contrast for Unsupervised Visual Represen.pdf:application/pdf;arXiv.org Snapshot:/home/clara/Zotero/storage/MWFZ6PHG/1911.html:text/html},
}

@misc{shen_learning_2022,
	title = {Learning {Co}-segmentation by {Segment} {Swapping} for {Retrieval} and {Discovery}},
	url = {http://arxiv.org/abs/2110.15904},
	doi = {10.48550/arXiv.2110.15904},
	abstract = {The goal of this work is to efficiently identify visually similar patterns in images, e.g. identifying an artwork detail copied between an engraving and an oil painting, or recognizing parts of a night-time photograph visible in its daytime counterpart. Lack of training data is a key challenge for this co-segmentation task. We present a simple yet surprisingly effective approach to overcome this difficulty: we generate synthetic training pairs by selecting segments in an image and copy-pasting them into another image. We then learn to predict the repeated region masks. We find that it is crucial to predict the correspondences as an auxiliary task and to use Poisson blending and style transfer on the training pairs to generalize on real data. We analyse results with two deep architectures relevant to our joint image analysis task: a transformer-based architecture and Sparse Nc-Net, a recent network designed to predict coarse correspondences using 4D convolutions. We show our approach provides clear improvements for artwork details retrieval on the Brueghel dataset and achieves competitive performance on two place recognition benchmarks, Tokyo247 and Pitts30K. We also demonstrate the potential of our approach for unsupervised image collection analysis by introducing a spectral graph clustering approach to object discovery and demonstrating it on the object discovery dataset of {\textbackslash}cite\{rubinstein2013unsupervised\} and the Brueghel dataset. Our code and data are available at http://imagine.enpc.fr/{\textasciitilde}shenx/SegSwap/.},
	urldate = {2024-08-14},
	publisher = {arXiv},
	author = {Shen, Xi and Efros, Alexei A. and Joulin, Armand and Aubry, Mathieu},
	month = mar,
	year = {2022},
	note = {arXiv:2110.15904 [cs]
version: 2},
	keywords = {eda},
	annote = {Comment: add results of unsupervised saliency detection},
	file = {arXiv Fulltext PDF:/home/clara/Zotero/storage/Z5N9JS7D/Shen et al. - 2022 - Learning Co-segmentation by Segment Swapping for R.pdf:application/pdf;arXiv.org Snapshot:/home/clara/Zotero/storage/CKZP3PLS/2110.html:text/html},
}

@misc{rees_cosmic_2013,
	title = {Cosmic {Origami} and {What} {We} {Don}'t {Know}},
	url = {https://onbeing.org/programs/martin-rees-cosmic-origami-and-what-we-dont-know/},
	abstract = {Parallel realities and the deep structure of space-time sound like science fiction. But these are matters of real scientific inquiry. Lord Martin Rees is an astrophysicist and atheist who spends his life contemplating such things.},
	language = {en-US},
	urldate = {2024-07-14},
	author = {Rees, Martin},
	month = nov,
	year = {2013},
	keywords = {edit},
	file = {Snapshot:/home/clara/Zotero/storage/NFW72BX6/martin-rees-cosmic-origami-and-what-we-dont-know.html:text/html},
}

@misc{noauthor_introduction_nodate,
	title = {Introduction • {Docs} • {Svelte}},
	url = {https://svelte.dev/docs/introduction},
	abstract = {Introduction • Svelte documentation},
	language = {en},
	urldate = {2024-08-14},
	keywords = {edit},
	file = {Snapshot:/home/clara/Zotero/storage/QKGAH8FC/introduction.html:text/html},
}

@misc{noauthor_draft_nodate,
	title = {{DRaFT} download},
	url = {https://www.greekmath.org/draft/draft_index.html},
	urldate = {2024-08-14},
	keywords = {edit},
	file = {DRaFT download:/home/clara/Zotero/storage/TI54XMYR/draft_index.html:text/html},
}

@article{de_young_diagrams_2009,
	title = {Diagrams in ancient {Egyptian} geometry: {Survey} and assessment},
	volume = {36},
	issn = {0315-0860},
	shorttitle = {Diagrams in ancient {Egyptian} geometry},
	url = {https://www.sciencedirect.com/science/article/pii/S0315086009000081},
	doi = {10.1016/j.hm.2009.02.004},
	abstract = {This article surveys and catalogs the geometric diagrams that survive from ancient Egypt. These diagrams are often overspecified and some contain inaccuracies in their construction. The diagrams accompany algorithmic texts and support the mathematical programme of their authors. The study concludes with a brief comparison with the diagram traditions of ancient Babylon, early India, and Greece.
Résumé
Cet article étudie les diagrammes géométriques qui survivent dans l’Egypte antique. Ces diagrammes sont plus spécifiques qu’il n’est nécessaire pour le problème et certains contiennent des inexactitudes dans leur construction. Les diagrammes accompagnent des textes algorithmiques et soutiennent le programme mathématique de leurs auteurs. L’étude se termine par une comparaison brève aux traditions diagrammatiques à Babylone et en Grèce.},
	number = {4},
	urldate = {2024-08-14},
	journal = {Historia Mathematica},
	author = {De Young, Gregg},
	month = nov,
	year = {2009},
	keywords = {edit},
	pages = {321--373},
	file = {ScienceDirect Snapshot:/home/clara/Zotero/storage/7MPQ6F86/S0315086009000081.html:text/html},
}

@misc{harris_svelte_2019,
	title = {Svelte 3: {Rethinking} reactivity},
	shorttitle = {Svelte 3},
	url = {https://svelte.dev/blog/svelte-3-rethinking-reactivity},
	language = {en},
	urldate = {2024-08-16},
	author = {Harris, Rich},
	month = apr,
	year = {2019},
	keywords = {edit},
	file = {Snapshot:/home/clara/Zotero/storage/HPT6HWFK/svelte-3-rethinking-reactivity.html:text/html},
}

@misc{kermorvant_teklia_nodate,
	title = {Teklia - {Automatic} {Text} {Recognition} - {The} convergence between {OCR} and {HTR} technologies},
	url = {https://teklia.com/blog/202212-atr/},
	urldate = {2024-08-17},
	author = {Kermorvant, Christopher},
	keywords = {ia},
	file = {Teklia - Automatic Text Recognition - The convergence between OCR and HTR technologies:/home/clara/Zotero/storage/ARLJPESS/202212-atr.html:text/html},
}

@article{bourdeloie_ce_2014,
	title = {Ce que le numérique fait aux sciences humaines et sociales.},
	copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://journals.openedition.org/ticetsociete/1500#tocto2n7},
	doi = {10.4000/ticetsociete.1500},
	abstract = {Les sciences humaines et sociales (SHS) ont pendant longtemps été l’objet de débats s’agissant de leur nature scientifique, de la posture du chercheur (neutre, engagée, critique), de leur essence prétendue subjective ou encore du rôle des méthodes et outils mobilisés… Toutefois, dans le contexte du numérique, les enjeux se sont déplacés et les débats renouvelés. Les SHS sont sujettes à de profondes transformations du fait des nouvelles méthodes qu’elles déploient, des nouvelles conditions pratiques de recherche auxquelles elles sont confrontées et de leur nouvel enracinement dans le monde social. Si on peut se réjouir des bienfaits du numérique pour les SHS, un regard critique s’impose à propos des caractéristiques des outils qui sont mobilisés et des valeurs que ces outils portent en eux.},
	language = {fr},
	number = {Vol. 7, N° 2},
	urldate = {2024-08-20},
	journal = {tic\&société},
	author = {Bourdeloie, Hélène},
	month = may,
	year = {2014},
	note = {Number: Vol. 7, N° 2
Publisher: ARTIC},
	keywords = {ia},
	file = {Full Text PDF:/home/clara/Zotero/storage/J2GC6USC/Bourdeloie - 2014 - Ce que le numérique fait aux sciences humaines et .pdf:application/pdf},
}

@article{guichard__2014,
	title = {« Éditorial », Épistémologies digitales des sciences humaines et sociales},
	copyright = {https://creativecommons.org/licenses/by-sa/4.0/},
	issn = {2271-6246},
	url = {https://journals.openedition.org/rsl/358},
	doi = {10.4000/rsl.358},
	abstract = {Le développement de l’internet a profondément changé les façons de faire de la recherche dans tous les domaines, y compris les sciences humaines. Par exemple, on voit un nombre croissant d’études se réclamant des humanités numériques (ou Digital Humanities pour reprendre le terme anglo-saxon fréquemment employé sans être traduit).  Les humanités numériques désigneraient un ensemble de pratiques visant à utiliser les médias ou les outils numériques pour analyser des données et/ou diffuser les ...},
	language = {fr},
	number = {2},
	urldate = {2024-08-20},
	journal = {Revue Sciences/Lettres},
	author = {Guichard, Éric and Poibeau, Thierry},
	month = feb,
	year = {2014},
	note = {Number: 2
Publisher: École normale supérieure},
keywords = {ia},
	file = {Full Text PDF:/home/clara/Zotero/storage/RRNRPL22/Guichard et Poibeau - 2014 - Éditorial.pdf:application/pdf},
}

@article{guichard_linternet_2014,
	title = {L’internet et les épistémologies des sciences humaines et sociales},
	copyright = {https://creativecommons.org/licenses/by-sa/4.0/},
	issn = {2271-6246},
	url = {https://journals.openedition.org/rsl/389#tocto3n3},
	doi = {10.4000/rsl.389},
	abstract = {Les ordinateurs et les réseaux nous rappellent à quel point notre pensée est instrumentée et nous font prendre conscience qu’elle l’a toujours été. Pour le dire autrement : la culture propre à l’informatique apparaît surtout technique. Mais elle n’est que la traduction contemporaine de l’ensemble des savoir-faire liés à la maîtrise de l’écriture. Nous (re)découvrons alors un lien étroit entre culture technique et culture des savants et des érudits, et les anthropologues ont montré la relation entre cette dernière et la culture au sens large : par effet de domination (le pouvoir de l’écrit) et parce que l’écriture invite à la réflexion sur les objets qu’elle manipule ou met en évidence. Il y a donc un lien direct entre culture technique propre à l’écriture et culture d’une société. Nous montrons alors comment l’écriture électronique et en réseau infléchit les problématiques et les épistémologies de disciplines communément regroupées sous l’étiquette « sciences humaines et sociales » (SHS) : nouvelles méthodes, potentialités combinatoires, questions posées par les usages du « numérique », etc., mais aussi savoir-faire élémentaires (écrire ou repérer un signe dans un texte). Certaines de ces problématiques commencent à être abordées par des personnes qui se revendiquent du mouvement des « humanités numériques ». Nous montrons que la faiblesse argumentative des représentants de ce mouvement est moins préoccupante pour les scientifiques que la facilité avec laquelle ils se font entendre : outre le dévoilement sociologique du monde universitaire actuel, toujours instructif, ce n’est pas tant l’essor des « humanités numériques » qui pose problème (parce qu’elles seraient mal définies ou joueraient d’un oxymore peu efficace épistémologiquement) que le silence de représentants des « SHS » quant à l’évolution des contours de chacune de leur discipline sous l’effet de l’écriture contemporaine. Pourtant, l’étude de cet effet, déjà balisée par des épistémologues, est prometteuse. Et elle permet de comprendre ce qui se « fabrique », de façon profane comme savante, en matière de culture numérique.},
	language = {fr},
	number = {2},
	urldate = {2024-08-20},
	journal = {Revue Sciences/Lettres},
	author = {Guichard, Éric},
	month = feb,
	year = {2014},
	note = {Number: 2
Publisher: École normale supérieure},
	keywords = {ia},
	file = {Full Text PDF:/home/clara/Zotero/storage/FATF7DBK/Guichard - 2014 - L’internet et les épistémologies des sciences huma.pdf:application/pdf},
}

@phdthesis{reynaud_diagrammes_2017,
	type = {These en préparation},
	title = {Les diagrammes mathématiques  paléo-babyloniens :  catalogue, propriétés, materielles, rôles dans les raisonnements},
	copyright = {Licence Etalab},
	shorttitle = {Les diagrammes mathématiques  paléo-babyloniens},
	url = {https://theses.fr/s197780},
	urldate = {2024-08-26},
	school = {Université Paris Cité},
	author = {Reynaud, Adeline},
	collaborator = {Proust, Christine and Lion, Brigitte},
	year = {2017},
	keywords = {edit},
}

@conference{conference2023,
  title        = {L'Open Data a-t-il tenu ses promesse~?},
  year         = {2023},
  address      = {Paris, France},
  month        = {12},
  organization = {ADEMEC},
  keywords = {ia},
}

@misc{appleby_presentation_nodate,
	title = {Presentation {API} 3.0},
	url = {https://iiif.io/api/presentation/3.0/},
	abstract = {Presentation API 3.0},
	language = {en},
	urldate = {2024-09-01},
	journal = {IIIF},
	author = {Appleby, Micheal and Crane, Tom and Sanderson, Robert and Stroop, Jon and Warner, Simeon},
	file = {Snapshot:/home/clara/Zotero/storage/F7FTM4RW/3.0.html:text/html},
	keywords = {eda},
}
