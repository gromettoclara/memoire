\begin{kwote}
``La notion de pensée spirituelle n’a pas de sens et ce que l’on croit relever d’une aptitude intellectuelle extraordinaire consiste, peut-être à 95 \%, en une maîtrise de notre système de signes et de ses combinaisons, qui peut certes confiner à l’art (la possibilité d’enchaîner des centaines, voire des milliers de gestes, d’algorithmes, de recettes ou de formules) mais qui reste majoritairement technique~: une somme d’apprentissages tout à fait accessibles et qui conduisent à des pratiques et des gestes enchaînés de façon de plus en plus rapide avec leur répétition. Nous retrouvons là les propos de Leibniz, de Dagognet et de Granger.''\footcite{guichard_linternet_2014}
\end{kwote}

Guichard souligne ici que la pensée s'ancre profondément dans la technique. À l'heure du numérique, chaque projet de recherche, avec ses sources et ses questions, a des besoins épistémologiques précis qui s’incarnent dans des structures informatiques particulières~: des protocoles, des formats, des dispositifs, des visualisations de données, etc. Au cœur de cette idiosyncrasie, comment créer des outils numériques qui s'inscrivent dans un écosystème plus large, tout en restant attentif aux spécificités de chaque objet et de chaque question de recherche~?

Cette question est centrale dans la construction d'une chaîne de traitement des sources. Le mythe du savant isolé, reclus dans son observatoire, penché sur ses grimoires, fait place à la réalité du collectif. La collaboration est désormais de mise~: pour favoriser le partage et l'accès aux données, pour garantir la reproductibilité des résultats et assurer la cohérence des pratiques. Malgré cela, la prolifération des outils spécialisés, souvent conçus pour un projet particulier, persiste dans le domaine des Humanités Numériques. Non seulement nécessitent-ils la mobilisation de ressources humaines et techniques importantes, mais aussi se révèlent-ils difficilement maintenables sur le long terme. Devant ces constats, la question se pose~: comment mettre en place des formes de mutualisation sur le plan technique~? Cet enjeu se joue à plusieurs niveaux~: ouverture des silos de donnée, scripting, architecture applicative et infrastructure hardware, protocoles pour diffuser et maintenir ces outils, pour collaborer autour de leur développement\ldots

Le principal défi réside dans la conciliation des exigences de généralité et de spécificité, pour créer des outils numériques qui non seulement répondent aux besoins spécifiques du projet qui les porte, mais aussi servent une communauté scientifique plus large et interdisciplinaire. Comment concevoir des systèmes suffisamment flexibles sans sacrifier leur pertinence et leur efficacité~? 

Ce mémoire propose une exploration de ces problématiques en s'appuyant sur un cas d'étude~: la construction de la plateforme \aikon, qui met à disposition des outils basés sur la \cv pour l'enrichissement, la sémantification et l'analyse des données visuelles. 

\section{Mise en contexte}

\subsection{Un changement de paradigme}

La numérisation massive de sources archivistiques et bibliographiques a radicalement transformé le paysage de la recherche en sciences humaines. Les ressources numériques et leurs usages se cessent de se développer et se diversifier. Les ouvrages et les manuscrits scientifiques, notamment, constituent une source essentielle pour la connaissance de l’histoire des sciences.

\begin{kwote}
``The availability of large amounts of digitized historical documents opened the door to the use of computational approaches for their analysis.''\footcite[p.2]{buttner_cordeep_2022}
\end{kwote}

La disponibilité des données numérisées est au fondement de la démarche basée sur des traitements automatiques des sources par des algorithmes de vision artificielle. Ces immenses corpus offrent des opportunités inédites pour comprendre les phénomènes culturels, sociaux et historiques à une échelle sans précédent, soulevant cependant des défis techniques et méthodologiques~: en premier lieu l'accès à la donnée, en second lieu, sa sémantification.  

Le décloisonnement des silos d'information est un enjeu majeur de la gouvernance des données, et le premier pas vers leur exploitation. Le concept d'\textit{open-data} renvoie à une ambition d'ouverture des données pour leur libre circulation et exploitation. Il concerne à la fois la diffusion des sources numériques auprès d'un public très large\footnote{Auprès de la communauté scientifique comme du grand public.} et l'enrichissement des ressources par la communauté scientifique. Le recours à des standards communs et des mécanismes d'échange normalisés est essentiel pour garantir l'interopérabilité des données et faciliter leur intégration dans des diverses infrastructures. En effet, la capacité à intégrer des données provenant de sources multiples et hétérogènes est indispensable pour exploiter pleinement le potentiel des ressources numérisées dans la recherche en \textsc{SHS}. En d'autres termes, il s'agit de briser les silos applicatifs pour concrétiser une \textit{harmonisation virtuelle}, une unification, créant ainsi un écosystème numérique collaboratif, où les données peuvent circuler librement entre les lieux de stockage. 

Au cœur des enjeux de l'open-data, la valorisation de la donnée brute reste en outre un défi à relever. Pour tirer pleinement parti de cette richesse disponible, il est nécessaire de mettre en place des infrastructures adaptées pour les mettre à disposition afin de favoriser la collaboration entre les différents acteurs (producteurs comme réutilisateur.rices). D'où la nécessité d'enrichir la donnée brute, pour la ``faire parler''. 

\begin{kwote}
``Thus, Big Data means to acquire, store, and analyze large amount of data that are generated quickly and are not always structured {[}\ldots{]}.''\footcite[p.26]{klinke_big_2016}
\end{kwote}

Après l'étape de numérisation, le document reste à l'état d'images matricielles directement issues du scan ou de la photographie des pages. Son contenu sémantique, c'est-à-dire le texte et les illustrations, demeure illisible aux machines, et donc caché au lecteur jusqu'à ce qu'il ouvre et lise la copie numérique. L'enjeu est alors de transformer ces images matricielles en données structurées, d'en obtenir de nouvelles représentations sémantiquement riches et manipulables, afin de construire des portails de bases de données permettant une interrogation fine du contenu des documents. 

La numérisation initie donc un processus de transformation de la donnée brute en informations structurées grâce à sa segmentation. Cette étape, en conférant un niveau d'abstraction supérieur aux données, les rend aptes à supporter de nouveaux traitements algorithmiques et des analyses scientifiques. Face à l'explosion des volumes de données, l'automatisation de ces analyses devient indispensable pour appréhender de nouveaux ordres de grandeur et extraire des connaissances.

\begin{kwote}
``[The] interpretation of results is still exclusive to humans, but computers can help us with the steps leading to that destination.''\footcite[p.26]{klinke_big_2016}
\end{kwote}

Des outils numériques vont permettre de décoder et d'interpréter les informations visuelles, faisant ainsi ``parler'' les données brutes~:

\subsection{De nouveaux outils}

Les Humanités Numériques, initialement concentrées sur l'étude du texte, ont bénéficié de l'essor des technologies d'extraction du texte. La Reconnaissance Optique de Caractères (\ocr), puis la Reconnaissance d'Écriture Manuscrite (\htr), ont suscité un intérêt croissant depuis les années 1950, mais c'est à partir des années 1990 que leur développement s'est véritablement accéléré. L'objectif premier est d'automatiser l'extraction de textes à partir d'images numériques de sources historiques. Confrontées à la diversité des polices, des mises en page et des orthographes rencontrées dans les documents, les méthodes se sont récemment tournées vers l'apprentissage profond (\dl). 

Cependant les archives et les sources contiennent aussi un grand nombre d'images. En comparaison de l'intérêt pour l'\htr et l'\ocr, le développement des réseaux de neurones et du \ml pour le traitement automatique du matériau visuel accompagnant le texte est relativement récent. Pourtant, les deux champs partagent une base commune~: le traitement du texte numérisé comme de ses illustrations, en apprentissage machine, revient à manipuler une image matricielle, soit une grille de pixels, et extraire puis encoder des informations sémantiques. 

\subsection{Perspectives ouvertes pour l'image}

Cette convergence technologique offre aujourd'hui la possibilité d'identifier et classifier des motifs récurrents au sein de vastes collections d'images, permettant une exploration plus systématique et à plus grande échelle des éléments visuels présents dans les sources. Les réseaux de neurones profonds ouvrent de nouvelles voies d'interrogation des archives numériques au prisme de leurs illustrations. 

\begin{kwote}
``They [les réseaux de neurone] open up a part of the digital archive for large-scale analysis, which, until now, has been left uncovered~: the millions of images in digitized books, newspapers, periodicals, and historical documents. As a result, they allow us to explore the visual side of the digital turn in historical research. Using these techniques, we can explore visual material in archives using nontextual search methods. Scholars can, for example, find visual material related to a particular topic, or, they can indentify transitions in the use of a particular medium, such as illustrations and photographs.''\footcite[p.2]{wevers_visual_2020}
\end{kwote}

Les représentations sémantiques extraites par les modèles de vision artificielle fournissent un niveau d'abstraction supérieur à l'image, visent à rendre son contenu compréhensible par la machine, offrant ainsi de nouvelles perspectives pour l'analyse et l'exploitation des données de la recherche.

\subsection{Structurer l'information}

L'extraction et l'encodage de texte ouvrent de nouvelles perspectives pour mettre en œuvre des chaînes d'édition partiellement automatisées. Or les formats de l'édition numériques permettent l'introduction d'une approche sémantique de l'exploitation des textes. Au-delà de la simple mise en forme, l'édition numérique implique désormais un balisage sémantique, basé sur des langages comme \xml et \html, et sur des standards comme \tei, rendant les contenus lisibles aussi bien par les humains que par les machines. Ainsi, l'édition ne se limite plus à la création d'un objet matériel, mais vise à structurer les contenus pour favoriser leur exploration et l'ouverture à de nouveaux traitements algorithmiques.

Comment étendre cette structuration aux images, de manière à les rendre exploitables par des traitements algorithmiques~? Les algorithmes de vectorisation automatique permettent de transformer des images géométriques simples, comme les diagrammes astronomiques qui constituent le corpus d'\eida, en données structurées. Cette représentation vectorielle offre un équivalent visuel du balisage sémantique appliqué aux textes, permettant ainsi de capturer le sens mathématique inhérent à ces images.

\subsection{Les données pour des ponts interdisciplinaires}

La combinaison de deux facteurs, l'augmentation exponentielle de la puissance de calcul et de la disponibilité des données visuelles, a permis à la \textit{Computer Vision} d'évoluer rapidement, permettant de créer des réseaux de neurones plus profonds, plus précis, et améliorant la vitesse et l'exactitude de tâches de plus en plus complexes\footcite{klinke_big_2016}. Mais cet avancement des techniques est aussi catalysée par leur application concrète et en contexte, car leur développement repose sur la disponibilité de larges \textit{datasets} annotés pour l'entraînement des modèles et l'évaluation des résultats. Ainsi si l'\ia profite au domaine de la recherche historique sur du matériel visuel, la réciproque est aussi vraie~: 

\begin{kwote}
``humanities could likewise be a boon to the development of more accurate and more sophisticated computer vision techniques. As classification algorithms have been trained on manually tagged sets, structural biases in their classification schemes will be reproduced in the results produced by computer vision techniques. In collaboration with humanities scholars, computer scientists could critically engage with these biases and rethink the way we annotate data sets and measure algorithmic accuracy.''\footcite[p.12]{wevers_visual_2020}
\end{kwote}

La mise à disposition de jeux de données sélectionnés et annotés par des experts améliore les performances et l'exactitude des modèles, faisant rapidement avancer la recherche en \cv. Ainsi le \dl, gourmand en données, profite de l'existence de ces corpus divers et complexes annotés par les chercheurs en \shs pour l'entraînement des modèles, et les historien.nes profitent du changement d'échelle de l'analyse permise par la \cv. La recherche et la technologie rentrent en dialogue, l'une répondant aux besoins de l'autre, créant des collaborations interdisciplinaires bénéfiques. D'où le besoin d'autant plus prégnant dans le domaine du \ml de penser les outils dans le sens de l'ouverture des systèmes afin de garantir l'interopérabilité de ces données annotées.

\subsection{\emph{Pipelines} et \emph{workflows}~: spécificités de l'implémentation de l'IA}

Les problématiques liées à l'intelligence artificielle ne se limitent pas à la construction de modèles performants. Son utilisation soulève également des questions en matière d'architecture matérielle et applicative. Déjà, les modèles d'\ia, particulièrement ceux basés sur l'apprentissage profond, nécessitent la puissance de calcul et les infrastructures \textit{hard-ware} adaptées. L'intégration de ces modèles dans des systèmes applicatifs exige également des algorithmes optimisés pour la parallélisation, ainsi que des solutions pour la gestion des données à grande échelle et pour la réduction de la latence. 

L'intégration de l'\ia dans un processus de traitement de données implique en outre la création de \textit{workflows} itératifs. Ces \textit{workflows} nécessitent une récupération des données en sortie, suivie d'une intervention humaine pour leur correction et leur réintégration dans le processus d'entraînement des modèles. Ces pipelines impliquent une boucle de rétroaction où les données produites par les modèles sont régulièrement évaluées, corrigées par des experts, puis réinjectées dans le processus d'apprentissage.

Alors comment préserver le rôle des chercheur.ses dans ce processus afin de garantir la qualité des résultats~? Comment concevoir des infrastructures applicatives ou logicielles capables de gérer ces flux de données~? Et est-il possible de créer un outil qui permette d'appliquer une méthode unifiée à des ensembles de données hétérogènes, un outil suffisamment flexible et adapté à divers traitements basés sur le \ml~? La réponse à ces questions nécessite une approche transversale, combinant des compétences en informatique, en développement applicatif, et en gestion de la donnée. 

Les enjeux et interrogations qui entourent la mise en place de ce \textit{workflow} m'ont occupé pendant mon stage, et constituent le cœur de ce qui est exposé dans ce mémoire. 

\section{Mission de stage}

La plateforme \aikon -- anciennement \eida -- est déjà dotée d’une chaîne de traitement partiellement automatisée qui intègre des fonctionnalités d’extraction et de recherche de similarités. Ces traitements permettent de repérer et de comparer des éléments visuels, en l'occurrence des diagrammes, au sein d’une base de données constituée d’images numérisées provenant de sources variées~: institutions de conservation ou collections personnelles.

La prochaine étape dans le développement de la plateforme consiste en l'implémentation de la fonctionnalité de vectorisation des diagrammes extraits, visant à affiner encore le niveau de structuration de l’information. Ce processus transforme une image en un ensemble de formes géométriques élémentaires, appelées primitives. Cette représentation, particulièrement adaptée au traitement informatique, permet de traduire les informations visuelles en structures mathématiques, facilitant ainsi leur manipulation, leur analyse et leur exploitation par des méthodes computationnelles.

Mon stage au sein de l'équipe d'histoire des sciences du laboratoire \syrte de l'Observatoire de Paris a consisté à développer un module de vectorisation automatique dans la plateforme \aikon, tout en participant à une réflexion plus large sur l'architecture de cette plateforme. En favorisant une approche modulaire et flexible, son évolution permettra d'intégrer plus facilement de nouvelles fonctionnalités et d'ouvrir la plateforme à d'autres domaines de recherche.

\section{Problématisation}

Comment concilier la nécessité de développer des outils génériques, réutilisables et performants avec la grande variété des données et des problématiques spécifiques à chaque étude~?

La tension s'exprime dans le défi de la personnalisation~: entre généralisation et spécificité. Nous explorerons comment cette dialectique se manifeste à chaque étape d'une chaîne de traitement. Au niveau de la description des données en premier lieu~: comment concilier la richesse des données réelles avec la nécessité de les structurer pour une exploitation efficace~? Par ailleurs, la construction de modèles de \cv s'inscrit au cœur de cette tension~: ils doivent être suffisamment généraux pour pouvoir s'adapter à la complexité du réel, tout en restant assez complexes et spécialisés sur des cas particuliers. Enfin, comment penser l'implémentation de ces modèles dans une plateforme évolutive et modulable~? 

Une question épistémologique sous-tendra tout au long de ce mémoire les question techniques~: comment l'outillage technique collaboratif contribue à façonner les méthodologies de la recherche~? 

\section{Annonce du plan}

Le développement se déclinera en trois partie. Premièrement, nous présenterons le projet \eida et ses objectifs, en le situant dans le paysage de la recherche en étude visuelles, et nous explorerons les enjeux qui découlent de cette inscription dans un contexte plus large. Nous aborderons ensuite les outils de \cv utilisés, les défis liés à leur mise en œuvre, et les perspectives ouvertes en terme d'édition numérique. Enfin, nous présenterons les contours d'une plateforme qui permettrait de rendre à la fois les méthodes et les résultats de l'analyse accessibles à la communauté scientifique, notamment à d'autres projets de recherche en études visuelles~; cette partie sera consacrée à la mise en œuvre technique d'une approche modulaire dans le développement d'une application web. 